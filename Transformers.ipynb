{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OeA8fMn9hrYI",
    "outputId": "0bad330d-5604-4e87-9b96-7fe337147013"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import typing\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomTokenizer:\n",
    "    \"\"\" Custom Tokenizer class to tokenize and detokenize text data into sequences of integers\n",
    "\n",
    "    Args:\n",
    "        split (str, optional): Split token to use when tokenizing text. Defaults to \" \".\n",
    "        char_level (bool, optional): Whether to tokenize at character level. Defaults to False.\n",
    "        lower (bool, optional): Whether to convert text to lowercase. Defaults to True.\n",
    "        start_token (str, optional): Start token to use when tokenizing text. Defaults to \"<start>\".\n",
    "        end_token (str, optional): End token to use when tokenizing text. Defaults to \"<eos>\".\n",
    "        filters (list, optional): List of characters to filter out. Defaults to\n",
    "            ['!', \"'\", '\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>',\n",
    "            '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n'].\n",
    "        filter_nums (bool, optional): Whether to filter out numbers. Defaults to True.\n",
    "        start (int, optional): Index to start tokenizing from. Defaults to 1.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            split: str=\" \",\n",
    "            char_level: bool=False,\n",
    "            lower: bool=True,\n",
    "            start_token: str=\"<start>\",\n",
    "            end_token: str=\"<eos>\",\n",
    "            filters: list = ['!', \"'\", '\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n'],\n",
    "            filter_nums: bool = True,\n",
    "            start: int=1,\n",
    "        ) -> None:\n",
    "        self.split = split\n",
    "        self.char_level = char_level\n",
    "        self.lower = lower\n",
    "        self.index_word = {}\n",
    "        self.word_index = {}\n",
    "        self.max_length = 0\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.filters = filters\n",
    "        self.filter_nums = filter_nums\n",
    "        self.start = start\n",
    "\n",
    "    @property\n",
    "    def start_token_index(self):\n",
    "        return self.word_index[self.start_token]\n",
    "\n",
    "    @property\n",
    "    def end_token_index(self):\n",
    "        return self.word_index[self.end_token]\n",
    "\n",
    "    def sort(self):\n",
    "        \"\"\" Sorts the word_index and index_word dictionaries\"\"\"\n",
    "        self.index_word = dict(enumerate(dict(sorted(self.word_index.items())), start=self.start))\n",
    "        self.word_index = {v: k for k, v in self.index_word.items()}\n",
    "\n",
    "    def split_line(self, line: str):\n",
    "        \"\"\" Splits a line of text into tokens\n",
    "\n",
    "        Args:\n",
    "            line (str): Line of text to split\n",
    "\n",
    "        Returns:\n",
    "            list: List of string tokens\n",
    "        \"\"\"\n",
    "        line = line.lower() if self.lower else line\n",
    "\n",
    "        if self.char_level:\n",
    "            return [char for char in line]\n",
    "\n",
    "        # split line with split token and check for filters\n",
    "        line_tokens = line.split(self.split)\n",
    "\n",
    "        new_tokens = []\n",
    "        for index, token in enumerate(line_tokens):\n",
    "            filtered_tokens = ['']\n",
    "            for c_index, char in enumerate(token):\n",
    "                if char in self.filters or (self.filter_nums and char.isdigit()):\n",
    "                    filtered_tokens += [char, ''] if c_index != len(token) -1 else [char]\n",
    "                else:\n",
    "                    filtered_tokens[-1] += char\n",
    "\n",
    "            new_tokens += filtered_tokens\n",
    "            if index != len(line_tokens) -1:\n",
    "                new_tokens += [self.split]\n",
    "\n",
    "        new_tokens = [token for token in new_tokens if token != '']\n",
    "\n",
    "        return new_tokens\n",
    "\n",
    "    def fit_on_texts(self, lines: typing.List[str]):\n",
    "        \"\"\" Fits the tokenizer on a list of lines of text\n",
    "        This function will update the word_index and index_word dictionaries and set the max_length attribute\n",
    "\n",
    "        Args:\n",
    "            lines (typing.List[str]): List of lines of text to fit the tokenizer on\n",
    "        \"\"\"\n",
    "        self.word_index = {key: value for value, key in enumerate([self.start_token, self.end_token, self.split] + self.filters)}\n",
    "\n",
    "        for line in tqdm(lines, desc=\"Fitting tokenizer\"):\n",
    "            line_tokens = self.split_line(line)\n",
    "            self.max_length = max(self.max_length, len(line_tokens) +2) # +2 for start and end tokens\n",
    "\n",
    "            for token in line_tokens:\n",
    "                if token not in self.word_index:\n",
    "                    self.word_index[token] = len(self.word_index)\n",
    "\n",
    "        self.sort()\n",
    "\n",
    "    def update(self, lines: typing.List[str]):\n",
    "        \"\"\" Updates the tokenizer with new lines of text\n",
    "        This function will update the word_index and index_word dictionaries and set the max_length attribute\n",
    "\n",
    "        Args:\n",
    "            lines (typing.List[str]): List of lines of text to update the tokenizer with\n",
    "        \"\"\"\n",
    "        new_tokens = 0\n",
    "        for line in tqdm(lines, desc=\"Updating tokenizer\"):\n",
    "            line_tokens = self.split_line(line)\n",
    "            self.max_length = max(self.max_length, len(line_tokens) +2) # +2 for start and end tokens\n",
    "            for token in line_tokens:\n",
    "                if token not in self.word_index:\n",
    "                    self.word_index[token] = len(self.word_index)\n",
    "                    new_tokens += 1\n",
    "\n",
    "        self.sort()\n",
    "        print(f\"Added {new_tokens} new tokens\")\n",
    "\n",
    "    def detokenize(self, sequences: typing.List[int], remove_start_end: bool=True):\n",
    "        \"\"\" Converts a list of sequences of tokens back into text\n",
    "\n",
    "        Args:\n",
    "            sequences (typing.list[int]): List of sequences of tokens to convert back into text\n",
    "            remove_start_end (bool, optional): Whether to remove the start and end tokens. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            typing.List[str]: List of strings of the converted sequences\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        for sequence in sequences:\n",
    "            line = \"\"\n",
    "            for token in sequence:\n",
    "                if token == 0:\n",
    "                    break\n",
    "                if remove_start_end and (token == self.start_token_index or token == self.end_token_index):\n",
    "                    continue\n",
    "\n",
    "                line += self.index_word[token]\n",
    "\n",
    "            lines.append(line)\n",
    "\n",
    "        return lines\n",
    "\n",
    "    def texts_to_sequences(self, lines: typing.List[str], include_start_end: bool=True):\n",
    "        \"\"\" Converts a list of lines of text into a list of sequences of tokens\n",
    "\n",
    "        Args:\n",
    "            lines (typing.list[str]): List of lines of text to convert into tokenized sequences\n",
    "            include_start_end (bool, optional): Whether to include the start and end tokens. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            typing.List[typing.List[int]]: List of sequences of tokens\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        for line in lines:\n",
    "            line_tokens = self.split_line(line)\n",
    "            sequence = [self.word_index[word] for word in line_tokens if word in self.word_index]\n",
    "            if include_start_end:\n",
    "                sequence = [self.word_index[self.start_token]] + sequence + [self.word_index[self.end_token]]\n",
    "\n",
    "            sequences.append(sequence)\n",
    "\n",
    "        return sequences\n",
    "\n",
    "    def save(self, path: str, type: str=\"json\"):\n",
    "        \"\"\" Saves the tokenizer to a file\n",
    "\n",
    "        Args:\n",
    "            path (str): Path to save the tokenizer to\n",
    "            type (str, optional): Type of file to save the tokenizer to. Defaults to \"json\".\n",
    "        \"\"\"\n",
    "        serialised_dict = self.dict()\n",
    "        if type == \"json\":\n",
    "            if os.path.dirname(path):\n",
    "                os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            with open(path, \"w\") as f:\n",
    "                json.dump(serialised_dict, f)\n",
    "\n",
    "    def dict(self):\n",
    "        \"\"\" Returns a dictionary of the tokenizer\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of the tokenizer\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"split\": self.split,\n",
    "            \"lower\": self.lower,\n",
    "            \"char_level\": self.char_level,\n",
    "            \"index_word\": self.index_word,\n",
    "            \"max_length\": self.max_length,\n",
    "            \"start_token\": self.start_token,\n",
    "            \"end_token\": self.end_token,\n",
    "            \"filters\": self.filters,\n",
    "            \"filter_nums\": self.filter_nums,\n",
    "            \"start\": self.start\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: typing.Union[str, dict], type: str=\"json\"):\n",
    "        \"\"\" Loads a tokenizer from a file\n",
    "\n",
    "        Args:\n",
    "            path (typing.Union[str, dict]): Path to load the tokenizer from or a dictionary of the tokenizer\n",
    "            type (str, optional): Type of file to load the tokenizer from. Defaults to \"json\".\n",
    "\n",
    "        Returns:\n",
    "            CustomTokenizer: Loaded tokenizer\n",
    "        \"\"\"\n",
    "        if isinstance(path, str):\n",
    "            if type == \"json\":\n",
    "                with open(path, \"r\") as f:\n",
    "                    load_dict = json.load(f)\n",
    "\n",
    "        elif isinstance(path, dict):\n",
    "            load_dict = path\n",
    "\n",
    "        tokenizer = CustomTokenizer()\n",
    "        tokenizer.split = load_dict[\"split\"]\n",
    "        tokenizer.lower = load_dict[\"lower\"]\n",
    "        tokenizer.char_level = load_dict[\"char_level\"]\n",
    "        tokenizer.index_word = {int(k): v for k, v in load_dict[\"index_word\"].items()}\n",
    "        tokenizer.max_length = load_dict[\"max_length\"]\n",
    "        tokenizer.start_token = load_dict[\"start_token\"]\n",
    "        tokenizer.end_token = load_dict[\"end_token\"]\n",
    "        tokenizer.filters = load_dict[\"filters\"]\n",
    "        tokenizer.filter_nums = bool(load_dict[\"filter_nums\"])\n",
    "        tokenizer.start = load_dict[\"start\"]\n",
    "        tokenizer.word_index = {v: int(k) for k, v in tokenizer.index_word.items()}\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    @property\n",
    "    def lenght(self):\n",
    "        return len(self.index_word)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!pip install mltu\n",
    "!pip install keras==2.11.0\n",
    "!pip install tensorflow==2.11.0\n",
    "!pip install tf2onnx==1.14.0\n",
    "!pip install onnx==1.12.0\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#config.py\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from mltu.configs import BaseModelConfigs\n",
    "\n",
    "\n",
    "class ModelConfigs(BaseModelConfigs):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_path = os.path.join(\n",
    "            \"/content/drive/MyDrive/Colab Notebooks/Models\", \"MyModel\")\n",
    "        self.num_layers = 4\n",
    "        self.d_model = 128\n",
    "        self.num_heads = 8\n",
    "        self.dff = 512\n",
    "        self.dropout_rate = 0.1\n",
    "        self.batch_size = 16\n",
    "        self.train_epochs = 20\n",
    "        # CustomSchedule parameters\n",
    "        self.init_lr = 0.00001\n",
    "        self.lr_after_warmup = 0.0005\n",
    "        self.final_lr = 0.0001\n",
    "        self.warmup_epochs = 2\n",
    "        self.decay_epochs = 18\n",
    "        # # Reduce the model to the simplest possible configuration\n",
    "        # self.num_layers = 1  # Minimum number of layers\n",
    "        # self.d_model = 16  # Very small model dimensionality\n",
    "        # self.num_heads = 1  # Only 1 attention head\n",
    "        # self.dff = 32  # Very small feed-forward network\n",
    "        # self.dropout_rate = 0.1  # Small dropout to keep things simple\n",
    "\n",
    "        # # Minimize batch size for fast processing\n",
    "        # self.batch_size = 2  # Tiny batch size for quick iteration\n",
    "\n",
    "        # # Train for only 1 epoch\n",
    "        # self.train_epochs = 1  # Just 1 epoch\n",
    "\n",
    "        # # Learning rate setup for quick processing\n",
    "        # self.init_lr = 0.01  # High learning rate to ensure quick changes, not crucial here\n",
    "        # self.lr_after_warmup = 0.01\n",
    "        # self.final_lr = 0.01\n",
    "        # self.warmup_epochs = 0  # Skip warmup entirely\n",
    "        # self.decay_epochs = 0  # No decay, since we're not really training\n",
    "\n",
    "        # Optionally, you can reduce the dataset size used in this single epoch\n",
    "        # This will be handled by your data provider and how you split your dataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#model.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from mltu.tensorflow.transformer.layers import Encoder, Decoder\n",
    "\n",
    "def Transformer(\n",
    "    input_vocab_size: int,\n",
    "    target_vocab_size: int,\n",
    "    encoder_input_size: int = None,\n",
    "    decoder_input_size: int = None,\n",
    "    num_layers: int=6,\n",
    "    d_model: int=512,\n",
    "    num_heads: int=8,\n",
    "    dff: int=2048,\n",
    "    dropout_rate: float=0.1,\n",
    "    ) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    A custom TensorFlow model that implements the Transformer architecture.\n",
    "\n",
    "    Args:\n",
    "        input_vocab_size (int): The size of the input vocabulary.\n",
    "        target_vocab_size (int): The size of the target vocabulary.\n",
    "        encoder_input_size (int): The size of the encoder input sequence.\n",
    "        decoder_input_size (int): The size of the decoder input sequence.\n",
    "        num_layers (int): The number of layers in the encoder and decoder.\n",
    "        d_model (int): The dimensionality of the model.\n",
    "        num_heads (int): The number of heads in the multi-head attention layer.\n",
    "        dff (int): The dimensionality of the feed-forward layer.\n",
    "        dropout_rate (float): The dropout rate.\n",
    "\n",
    "    Returns:\n",
    "        A TensorFlow Keras model.\n",
    "    \"\"\"\n",
    "    inputs = [\n",
    "        tf.keras.layers.Input(shape=(encoder_input_size,), dtype=tf.int64),\n",
    "        tf.keras.layers.Input(shape=(decoder_input_size,), dtype=tf.int64)\n",
    "        ]\n",
    "\n",
    "    encoder_input, decoder_input = inputs\n",
    "\n",
    "    encoder = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, vocab_size=input_vocab_size, dropout_rate=dropout_rate)(encoder_input)\n",
    "    decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, vocab_size=target_vocab_size, dropout_rate=dropout_rate)(decoder_input, encoder)\n",
    "\n",
    "    output = tf.keras.layers.Dense(target_vocab_size)(decoder)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#import keras\n",
    "#print(keras.version())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train.py\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "try: [tf.config.experimental.set_memory_growth(gpu, True) for gpu in tf.config.experimental.list_physical_devices(\"GPU\")]\n",
    "except: pass\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from mltu.tensorflow.callbacks import Model2onnx, WarmupCosineDecay\n",
    "\n",
    "from mltu.tensorflow.dataProvider import DataProvider\n",
    "from mltu.tokenizers import CustomTokenizer\n",
    "\n",
    "from mltu.tensorflow.transformer.utils import MaskedAccuracy, MaskedLoss\n",
    "from mltu.tensorflow.transformer.callbacks import EncDecSplitCallback\n",
    "\n",
    "#from model import Transformer\n",
    "#from configs import ModelConfigs\n",
    "\n",
    "configs = ModelConfigs()\n",
    "os.makedirs(configs.model_path, exist_ok=True)\n",
    "\n",
    "# Path to dataset\n",
    "en_training_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-train.en\"\n",
    "en_validation_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-dev.en\"\n",
    "es_training_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-train.fa\"\n",
    "es_validation_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-dev.fa\"\n",
    "\n",
    "def read_files(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        en_train_dataset = f.read().split(\"\\n\")[:-1]\n",
    "    return en_train_dataset\n",
    "\n",
    "en_training_data = read_files(en_training_data_path)\n",
    "en_validation_data = read_files(en_validation_data_path)\n",
    "es_training_data = read_files(es_training_data_path)\n",
    "es_validation_data = read_files(es_validation_data_path)\n",
    "\n",
    "#########################\n",
    "max_lenght = 500\n",
    "max_samples = 1000\n",
    "\n",
    "train_dataset = [[es_sentence, en_sentence] for es_sentence, en_sentence in zip(es_training_data, en_training_data) if len(es_sentence) <= max_lenght and len(en_sentence) <= max_lenght][:max_samples]\n",
    "val_dataset = [[es_sentence, en_sentence] for es_sentence, en_sentence in zip(es_validation_data, en_validation_data) if len(es_sentence) <= max_lenght and len(en_sentence) <= max_lenght][:max_samples]\n",
    "es_training_data, en_training_data = zip(*train_dataset)\n",
    "es_validation_data, en_validation_data = zip(*val_dataset)\n",
    "\n",
    "#########################\n",
    "\n",
    "# # Consider only sentences with length <= 500\n",
    "# max_lenght = 500\n",
    "# train_dataset = [[es_sentence, en_sentence] for es_sentence, en_sentence in zip(es_training_data, en_training_data) if len(es_sentence) <= max_lenght and len(en_sentence) <= max_lenght]\n",
    "# val_dataset = [[es_sentence, en_sentence] for es_sentence, en_sentence in zip(es_validation_data, en_validation_data) if len(es_sentence) <= max_lenght and len(en_sentence) <= max_lenght]\n",
    "# es_training_data, en_training_data = zip(*train_dataset)\n",
    "# es_validation_data, en_validation_data = zip(*val_dataset)\n",
    "\n",
    "# prepare spanish tokenizer, this is the input language\n",
    "tokenizer = CustomTokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(es_training_data)\n",
    "tokenizer.save(configs.model_path + \"/tokenizer.json\")\n",
    "\n",
    "# prepare english tokenizer, this is the output language\n",
    "detokenizer = CustomTokenizer(char_level=True)\n",
    "detokenizer.fit_on_texts(en_training_data)\n",
    "detokenizer.save(configs.model_path + \"/detokenizer.json\")\n",
    "\n",
    "\n",
    "# def preprocess_inputs(data_batch, label_batch):\n",
    "#     encoder_input = np.zeros((len(data_batch), tokenizer.max_length)).astype(np.int64)\n",
    "#     decoder_input = np.zeros((len(label_batch), detokenizer.max_length)).astype(np.int64)\n",
    "#     decoder_output = np.zeros((len(label_batch), detokenizer.max_length)).astype(np.int64)\n",
    "\n",
    "#     data_batch_tokens = tokenizer.texts_to_sequences(data_batch)\n",
    "#     label_batch_tokens = detokenizer.texts_to_sequences(label_batch)\n",
    "\n",
    "#     for index, (data, label) in enumerate(zip(data_batch_tokens, label_batch_tokens)):\n",
    "#         encoder_input[index][:len(data)] = data\n",
    "#         decoder_input[index][:len(label)-1] = label[:-1] # Drop the [END] tokens\n",
    "#         decoder_output[index][:len(label)-1] = label[1:] # Drop the [START] tokens\n",
    "\n",
    "#     return (encoder_input, decoder_input), decoder_output\n",
    "\n",
    "###################################\n",
    "def preprocess_inputs(data_batch, label_batch):\n",
    "    # Set maximum lengths based on the data\n",
    "    max_encoder_length = max(len(seq) for seq in tokenizer.texts_to_sequences(data_batch))\n",
    "    max_decoder_length = max(len(seq) for seq in detokenizer.texts_to_sequences(label_batch))\n",
    "\n",
    "    encoder_input = np.zeros((len(data_batch), max_encoder_length)).astype(np.int64)\n",
    "    decoder_input = np.zeros((len(label_batch), max_decoder_length)).astype(np.int64)\n",
    "    decoder_output = np.zeros((len(label_batch), max_decoder_length)).astype(np.int64)\n",
    "\n",
    "    data_batch_tokens = tokenizer.texts_to_sequences(data_batch)\n",
    "    label_batch_tokens = detokenizer.texts_to_sequences(label_batch)\n",
    "\n",
    "    for index, (data, label) in enumerate(zip(data_batch_tokens, label_batch_tokens)):\n",
    "        encoder_input[index][:len(data)] = data\n",
    "        decoder_input[index][:len(label)-1] = label[:-1]  # Drop the [END] tokens\n",
    "        decoder_output[index][:len(label)-1] = label[1:]  # Drop the [START] tokens\n",
    "\n",
    "    return (encoder_input, decoder_input), decoder_output\n",
    "\n",
    "###################################\n",
    "\n",
    "\n",
    "\n",
    "# Create Training Data Provider\n",
    "train_dataProvider = DataProvider(\n",
    "    train_dataset,\n",
    "    batch_size=configs.batch_size,\n",
    "    batch_postprocessors=[preprocess_inputs],\n",
    "    use_cache=True,\n",
    "    )\n",
    "\n",
    "# Create Validation Data Provider\n",
    "val_dataProvider = DataProvider(\n",
    "    val_dataset,\n",
    "    batch_size=configs.batch_size,\n",
    "    batch_postprocessors=[preprocess_inputs],\n",
    "    use_cache=True,\n",
    "    )\n",
    "\n",
    "# Create TensorFlow Transformer Model\n",
    "transformer = Transformer(\n",
    "    num_layers=configs.num_layers,\n",
    "    d_model=configs.d_model,\n",
    "    num_heads=configs.num_heads,\n",
    "    dff=configs.dff,\n",
    "    input_vocab_size=len(tokenizer)+1,\n",
    "    target_vocab_size=len(detokenizer)+1,\n",
    "    dropout_rate=configs.dropout_rate,\n",
    "    encoder_input_size=tokenizer.max_length,\n",
    "    decoder_input_size=detokenizer.max_length\n",
    "    )\n",
    "\n",
    "transformer.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=configs.init_lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# Compile the model\n",
    "transformer.compile(\n",
    "    loss=MaskedLoss(),\n",
    "    optimizer=optimizer,\n",
    "    metrics=[MaskedAccuracy()],\n",
    "    run_eagerly=False\n",
    "    )\n",
    "\n",
    "# Define callbacks\n",
    "warmupCosineDecay = WarmupCosineDecay(\n",
    "    lr_after_warmup=configs.lr_after_warmup,\n",
    "    final_lr=configs.final_lr,\n",
    "    warmup_epochs=configs.warmup_epochs,\n",
    "    decay_epochs=configs.decay_epochs,\n",
    "    initial_lr=configs.init_lr,\n",
    "    )\n",
    "earlystopper = EarlyStopping(monitor=\"val_masked_accuracy\", patience=5, verbose=1, mode=\"max\")\n",
    "# checkpoint = ModelCheckpoint(f\"{configs.model_path}/model_epoch-{{epoch:02d}}.h5\", monitor=\"val_masked_accuracy\", verbose=1, save_best_only=True, mode=\"max\", save_weights_only=False)\n",
    "checkpoint = ModelCheckpoint(f\"{configs.model_path}/model.h5\", monitor=\"val_masked_accuracy\", verbose=1, save_best_only=True, mode=\"max\", save_weights_only=False)\n",
    "tb_callback = TensorBoard(f\"{configs.model_path}/logs\")\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor=\"val_masked_accuracy\", factor=0.9, min_delta=1e-10, patience=2, verbose=1, mode=\"max\")\n",
    "model2onnx = Model2onnx(f\"{configs.model_path}/model.h5\", metadata={\"tokenizer\": tokenizer.dict(), \"detokenizer\": detokenizer.dict()}, save_on_epoch_end=False)\n",
    "encDecSplitCallback = EncDecSplitCallback(configs.model_path, encoder_metadata={\"tokenizer\": tokenizer.dict()}, decoder_metadata={\"detokenizer\": detokenizer.dict()})\n",
    "\n",
    "configs.save()\n",
    "\n",
    "import glob\n",
    "import glob\n",
    "\n",
    "# Find the latest checkpoint\n",
    "latest_checkpoint = max(glob.glob(f\"{configs.model_path}/model_epoch-*.h5\"), key=os.path.getctime, default=None)\n",
    "\n",
    "# If a checkpoint exists, load the model from it\n",
    "# if latest_checkpoint:\n",
    "#     print(f\"Loading model from checkpoint: {latest_checkpoint}\")\n",
    "#     transformer = tf.keras.models.load_model(latest_checkpoint, custom_objects={\"MaskedLoss\": MaskedLoss, \"MaskedAccuracy\": MaskedAccuracy})\n",
    "#     # Extract the epoch number from the checkpoint filename\n",
    "#     initial_epoch = int(latest_checkpoint.split('-')[-1].split('.')[0])\n",
    "# else:\n",
    "#     print(\"No checkpoint found, starting training from scratch.\")\n",
    "#     initial_epoch = 0\n",
    "\n",
    "# Train the model\n",
    "transformer.fit(\n",
    "    train_dataProvider,\n",
    "    validation_data=val_dataProvider,\n",
    "    epochs=configs.train_epochs,\n",
    "    callbacks=[\n",
    "        earlystopper,\n",
    "        warmupCosineDecay,\n",
    "        checkpoint,\n",
    "        tb_callback,\n",
    "        reduceLROnPlat,\n",
    "        model2onnx,\n",
    "        encDecSplitCallback\n",
    "        ],\n",
    "    # initial_epoch=initial_epoch  # Resume from the epoch of the last checkpoint\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#test.py\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from mltu.tokenizers import CustomTokenizer\n",
    "from mltu.inferenceModel import OnnxInferenceModel\n",
    "\n",
    "class PtEnTranslator(OnnxInferenceModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.new_inputs = self.model.get_inputs()\n",
    "        self.tokenizer = CustomTokenizer.load(self.metadata[\"tokenizer\"])\n",
    "        self.detokenizer = CustomTokenizer.load(self.metadata[\"detokenizer\"])\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        start = time.time()\n",
    "        tokenized_sentence = self.tokenizer.texts_to_sequences([sentence])[0]\n",
    "        encoder_input = np.pad(tokenized_sentence, (0, self.tokenizer.max_length - len(tokenized_sentence)), constant_values=0).astype(np.int64)\n",
    "\n",
    "        tokenized_results = [self.detokenizer.start_token_index]\n",
    "        for index in range(self.detokenizer.max_length - 1):\n",
    "            decoder_input = np.pad(tokenized_results, (0, self.detokenizer.max_length - len(tokenized_results)), constant_values=0).astype(np.int64)\n",
    "            input_dict = {\n",
    "                self.model._inputs_meta[0].name: np.expand_dims(encoder_input, axis=0),\n",
    "                self.model._inputs_meta[1].name: np.expand_dims(decoder_input, axis=0),\n",
    "            }\n",
    "            preds = self.model.run(None, input_dict)[0] # preds shape (1, 206, 29110)\n",
    "            pred_results = np.argmax(preds, axis=2)\n",
    "            tokenized_results.append(pred_results[0][index])\n",
    "\n",
    "            if tokenized_results[-1] == self.detokenizer.end_token_index:\n",
    "                break\n",
    "\n",
    "        results = self.detokenizer.detokenize([tokenized_results])\n",
    "        return results[0], time.time() - start\n",
    "\n",
    "def read_files(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        en_train_dataset = f.read().split(\"\\n\")[:-1]\n",
    "    return en_train_dataset\n",
    "\n",
    "# Path to dataset\n",
    "en_validation_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-dev.en\"\n",
    "es_validation_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-dev.fa\"\n",
    "\n",
    "en_validation_data = read_files(en_validation_data_path)\n",
    "es_validation_data = read_files(es_validation_data_path)\n",
    "\n",
    "# Consider only sentences with length <= 500\n",
    "max_lenght = 500\n",
    "val_examples = [[es_sentence, en_sentence] for es_sentence, en_sentence in zip(es_validation_data, en_validation_data) if len(es_sentence) <= max_lenght and len(en_sentence) <= max_lenght]\n",
    "\n",
    "translator = PtEnTranslator(\"/content/drive/MyDrive/Colab Notebooks/Models/MyModel/model.onnx\")\n",
    "\n",
    "val_dataset = []\n",
    "for es, en in val_examples:\n",
    "    results, duration = translator.predict(es)\n",
    "    print(\"Farsi:     \", es.lower())\n",
    "    print(\"English:     \", en.lower())\n",
    "    print(\"English pred:\", results)\n",
    "    print(duration)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hwrV8NwAKHai"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}