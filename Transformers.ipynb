{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "OeA8fMn9hrYI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643282951,
     "user_tz": -210,
     "elapsed": 2911,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "outputId": "2fdf4086-1301-4176-ea29-0f3f480d6b8e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "XGXeXopWgwCk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643288091,
     "user_tz": -210,
     "elapsed": 5142,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "outputId": "84b91af4-ebc3-4f29-fcac-74b8fcea9f98",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "996371\n",
      "1990\n",
      "('وسايلتو جمع کن باشه.', 'يادمه يه گاو اونجا بود', 'و رييس سازمان \"ياماگاتو\" هم بين کشته ها هستن')\n",
      "('Pack your stuff.', 'I remember the cow that stayed over there.', '_')\n"
     ]
    }
   ],
   "source": [
    "en_training_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-train.en\"\n",
    "en_validation_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-dev.en\"\n",
    "fa_training_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-train.fa\"\n",
    "fa_validation_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-dev.fa\"\n",
    "\n",
    "def read_files(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        en_train_dataset = f.read().split(\"\\n\")[:-1]\n",
    "    return en_train_dataset\n",
    "\n",
    "en_training_data = read_files(en_training_data_path)\n",
    "en_validation_data = read_files(en_validation_data_path)\n",
    "fa_training_data = read_files(fa_training_data_path)\n",
    "fa_validation_data = read_files(fa_validation_data_path)\n",
    "\n",
    "max_lenght = 500\n",
    "train_dataset = [[fa_sentence, en_sentence] for fa_sentence, en_sentence in zip(fa_training_data, en_training_data) if len(fa_sentence) <= max_lenght and len(en_sentence) <= max_lenght]\n",
    "val_dataset = [[fa_sentence, en_sentence] for fa_sentence, en_sentence in zip(fa_validation_data, en_validation_data) if len(fa_sentence) <= max_lenght and len(en_sentence) <= max_lenght]\n",
    "fa_training_data, en_training_data = zip(*train_dataset)\n",
    "fa_validation_data, en_validation_data = zip(*val_dataset)\n",
    "\n",
    "print(len(fa_training_data))\n",
    "print(len(fa_validation_data))\n",
    "print(fa_training_data[:3])\n",
    "print(en_training_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "jBgeUCCtjVQ7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643288092,
     "user_tz": -210,
     "elapsed": 15,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import typing\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomTokenizer:\n",
    "    \"\"\" Custom Tokenizer class to tokenize and detokenize text data into sequences of integers\n",
    "\n",
    "    Args:\n",
    "        split (str, optional): Split token to use when tokenizing text. Defaults to \" \".\n",
    "        char_level (bool, optional): Whether to tokenize at character level. Defaults to False.\n",
    "        lower (bool, optional): Whether to convert text to lowercase. Defaults to True.\n",
    "        start_token (str, optional): Start token to use when tokenizing text. Defaults to \"<start>\".\n",
    "        end_token (str, optional): End token to use when tokenizing text. Defaults to \"<eos>\".\n",
    "        filters (list, optional): List of characters to filter out. Defaults to\n",
    "            ['!', \"'\", '\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>',\n",
    "            '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n'].\n",
    "        filter_nums (bool, optional): Whether to filter out numbers. Defaults to True.\n",
    "        start (int, optional): Index to start tokenizing from. Defaults to 1.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            split: str=\" \",\n",
    "            char_level: bool=False,\n",
    "            lower: bool=True,\n",
    "            start_token: str=\"<start>\",\n",
    "            end_token: str=\"<eos>\",\n",
    "            filters: list = ['!', \"'\", '\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n'],\n",
    "            filter_nums: bool = True,\n",
    "            start: int=1,\n",
    "        ) -> None:\n",
    "        self.split = split\n",
    "        self.char_level = char_level\n",
    "        self.lower = lower\n",
    "        self.index_word = {}\n",
    "        self.word_index = {}\n",
    "        self.max_length = 0\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.filters = filters\n",
    "        self.filter_nums = filter_nums\n",
    "        self.start = start\n",
    "\n",
    "    @property\n",
    "    def start_token_index(self):\n",
    "        return self.word_index[self.start_token]\n",
    "\n",
    "    @property\n",
    "    def end_token_index(self):\n",
    "        return self.word_index[self.end_token]\n",
    "\n",
    "    def sort(self):\n",
    "        \"\"\" Sorts the word_index and index_word dictionaries\"\"\"\n",
    "        self.index_word = dict(enumerate(dict(sorted(self.word_index.items())), start=self.start))\n",
    "        self.word_index = {v: k for k, v in self.index_word.items()}\n",
    "\n",
    "    def split_line(self, line: str):\n",
    "        \"\"\" Splits a line of text into tokens\n",
    "\n",
    "        Args:\n",
    "            line (str): Line of text to split\n",
    "\n",
    "        Returns:\n",
    "            list: List of string tokens\n",
    "        \"\"\"\n",
    "        line = line.lower() if self.lower else line\n",
    "\n",
    "        if self.char_level:\n",
    "            return [char for char in line]\n",
    "\n",
    "        # split line with split token and check for filters\n",
    "        line_tokens = line.split(self.split)\n",
    "\n",
    "        new_tokens = []\n",
    "        for index, token in enumerate(line_tokens):\n",
    "            filtered_tokens = ['']\n",
    "            for c_index, char in enumerate(token):\n",
    "                if char in self.filters or (self.filter_nums and char.isdigit()):\n",
    "                    filtered_tokens += [char, ''] if c_index != len(token) -1 else [char]\n",
    "                else:\n",
    "                    filtered_tokens[-1] += char\n",
    "\n",
    "            new_tokens += filtered_tokens\n",
    "            if index != len(line_tokens) -1:\n",
    "                new_tokens += [self.split]\n",
    "\n",
    "        new_tokens = [token for token in new_tokens if token != '']\n",
    "\n",
    "        return new_tokens\n",
    "\n",
    "    def fit_on_texts(self, lines: typing.List[str]):\n",
    "        \"\"\" Fits the tokenizer on a list of lines of text\n",
    "        This function will update the word_index and index_word dictionaries and set the max_length attribute\n",
    "\n",
    "        Args:\n",
    "            lines (typing.List[str]): List of lines of text to fit the tokenizer on\n",
    "        \"\"\"\n",
    "        self.word_index = {key: value for value, key in enumerate([self.start_token, self.end_token, self.split] + self.filters)}\n",
    "\n",
    "        for line in tqdm(lines, desc=\"Fitting tokenizer\"):\n",
    "            line_tokens = self.split_line(line)\n",
    "            self.max_length = max(self.max_length, len(line_tokens) +2) # +2 for start and end tokens\n",
    "\n",
    "            for token in line_tokens:\n",
    "                if token not in self.word_index:\n",
    "                    self.word_index[token] = len(self.word_index)\n",
    "\n",
    "        self.sort()\n",
    "\n",
    "    def update(self, lines: typing.List[str]):\n",
    "        \"\"\" Updates the tokenizer with new lines of text\n",
    "        This function will update the word_index and index_word dictionaries and set the max_length attribute\n",
    "\n",
    "        Args:\n",
    "            lines (typing.List[str]): List of lines of text to update the tokenizer with\n",
    "        \"\"\"\n",
    "        new_tokens = 0\n",
    "        for line in tqdm(lines, desc=\"Updating tokenizer\"):\n",
    "            line_tokens = self.split_line(line)\n",
    "            self.max_length = max(self.max_length, len(line_tokens) +2) # +2 for start and end tokens\n",
    "            for token in line_tokens:\n",
    "                if token not in self.word_index:\n",
    "                    self.word_index[token] = len(self.word_index)\n",
    "                    new_tokens += 1\n",
    "\n",
    "        self.sort()\n",
    "        print(f\"Added {new_tokens} new tokens\")\n",
    "\n",
    "    def detokenize(self, sequences: typing.List[int], remove_start_end: bool=True):\n",
    "        \"\"\" Converts a list of sequences of tokens back into text\n",
    "\n",
    "        Args:\n",
    "            sequences (typing.list[int]): List of sequences of tokens to convert back into text\n",
    "            remove_start_end (bool, optional): Whether to remove the start and end tokens. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            typing.List[str]: List of strings of the converted sequences\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        for sequence in sequences:\n",
    "            line = \"\"\n",
    "            for token in sequence:\n",
    "                if token == 0:\n",
    "                    break\n",
    "                if remove_start_end and (token == self.start_token_index or token == self.end_token_index):\n",
    "                    continue\n",
    "\n",
    "                line += self.index_word[token]\n",
    "\n",
    "            lines.append(line)\n",
    "\n",
    "        return lines\n",
    "\n",
    "    def texts_to_sequences(self, lines: typing.List[str], include_start_end: bool=True):\n",
    "        \"\"\" Converts a list of lines of text into a list of sequences of tokens\n",
    "\n",
    "        Args:\n",
    "            lines (typing.list[str]): List of lines of text to convert into tokenized sequences\n",
    "            include_start_end (bool, optional): Whether to include the start and end tokens. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            typing.List[typing.List[int]]: List of sequences of tokens\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        for line in lines:\n",
    "            line_tokens = self.split_line(line)\n",
    "            sequence = [self.word_index[word] for word in line_tokens if word in self.word_index]\n",
    "            if include_start_end:\n",
    "                sequence = [self.word_index[self.start_token]] + sequence + [self.word_index[self.end_token]]\n",
    "\n",
    "            sequences.append(sequence)\n",
    "\n",
    "        return sequences\n",
    "\n",
    "    def save(self, path: str, type: str=\"json\"):\n",
    "        \"\"\" Saves the tokenizer to a file\n",
    "\n",
    "        Args:\n",
    "            path (str): Path to save the tokenizer to\n",
    "            type (str, optional): Type of file to save the tokenizer to. Defaults to \"json\".\n",
    "        \"\"\"\n",
    "        serialised_dict = self.dict()\n",
    "        if type == \"json\":\n",
    "            if os.path.dirname(path):\n",
    "                os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            with open(path, \"w\") as f:\n",
    "                json.dump(serialised_dict, f)\n",
    "\n",
    "    def dict(self):\n",
    "        \"\"\" Returns a dictionary of the tokenizer\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of the tokenizer\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"split\": self.split,\n",
    "            \"lower\": self.lower,\n",
    "            \"char_level\": self.char_level,\n",
    "            \"index_word\": self.index_word,\n",
    "            \"max_length\": self.max_length,\n",
    "            \"start_token\": self.start_token,\n",
    "            \"end_token\": self.end_token,\n",
    "            \"filters\": self.filters,\n",
    "            \"filter_nums\": self.filter_nums,\n",
    "            \"start\": self.start\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: typing.Union[str, dict], type: str=\"json\"):\n",
    "        \"\"\" Loads a tokenizer from a file\n",
    "\n",
    "        Args:\n",
    "            path (typing.Union[str, dict]): Path to load the tokenizer from or a dictionary of the tokenizer\n",
    "            type (str, optional): Type of file to load the tokenizer from. Defaults to \"json\".\n",
    "\n",
    "        Returns:\n",
    "            CustomTokenizer: Loaded tokenizer\n",
    "        \"\"\"\n",
    "        if isinstance(path, str):\n",
    "            if type == \"json\":\n",
    "                with open(path, \"r\") as f:\n",
    "                    load_dict = json.load(f)\n",
    "\n",
    "        elif isinstance(path, dict):\n",
    "            load_dict = path\n",
    "\n",
    "        tokenizer = CustomTokenizer()\n",
    "        tokenizer.split = load_dict[\"split\"]\n",
    "        tokenizer.lower = load_dict[\"lower\"]\n",
    "        tokenizer.char_level = load_dict[\"char_level\"]\n",
    "        tokenizer.index_word = {int(k): v for k, v in load_dict[\"index_word\"].items()}\n",
    "        tokenizer.max_length = load_dict[\"max_length\"]\n",
    "        tokenizer.start_token = load_dict[\"start_token\"]\n",
    "        tokenizer.end_token = load_dict[\"end_token\"]\n",
    "        tokenizer.filters = load_dict[\"filters\"]\n",
    "        tokenizer.filter_nums = bool(load_dict[\"filter_nums\"])\n",
    "        tokenizer.start = load_dict[\"start\"]\n",
    "        tokenizer.word_index = {v: int(k) for k, v in tokenizer.index_word.items()}\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    @property\n",
    "    def lenght(self):\n",
    "        return len(self.index_word)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "EIZ0sBEAjd5L",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643299729,
     "user_tz": -210,
     "elapsed": 11651,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "outputId": "8862ec10-561e-404d-e9e0-935e0e619206",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Fitting tokenizer: 100%|██████████| 996371/996371 [00:06<00:00, 147637.84it/s]\n",
      "Fitting tokenizer: 100%|██████████| 996371/996371 [00:04<00:00, 201185.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# prepare Spanish tokenizer, this is the input language\n",
    "tokenizer = CustomTokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(fa_training_data)\n",
    "tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "# prepare English tokenizer, this is the output language\n",
    "detokenizer = CustomTokenizer(char_level=True)\n",
    "detokenizer.fit_on_texts(en_training_data)\n",
    "detokenizer.save(\"detokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Tl-b_qmVjnAl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643299730,
     "user_tz": -210,
     "elapsed": 14,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "outputId": "ff2abde6-9d81-4945-b82b-6bdc01b4230a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[33, 51, 48, 55, 55, 58, 3, 66, 58, 61, 55, 47, 15, 3, 51, 58, 66, 3, 44, 61, 48, 3, 68, 58, 64, 36, 32]\n",
      "['<start>hello world, how are you?<eos>']\n",
      "['hello world, how are you?']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = detokenizer.texts_to_sequences([\"Hello world, how are you?\"])[0]\n",
    "print(tokenized_sentence)\n",
    "\n",
    "detokenized_sentence = detokenizer.detokenize([tokenized_sentence], remove_start_end=False)\n",
    "print(detokenized_sentence)\n",
    "\n",
    "detokenized_sentence = detokenizer.detokenize([tokenized_sentence])\n",
    "print(detokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "q7EFGwNFxZDR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643302381,
     "user_tz": -210,
     "elapsed": 2664,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "outputId": "44af1bf9-d810-4686-f98b-fa808547ce6b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: mltu in /usr/local/lib/python3.10/dist-packages (1.2.5)\n",
      "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.10/dist-packages (from mltu) (6.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mltu) (4.66.5)\n",
      "Requirement already satisfied: qqdm==0.0.7 in /usr/local/lib/python3.10/dist-packages (from mltu) (0.0.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mltu) (2.1.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mltu) (1.26.4)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from mltu) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from mltu) (9.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from mltu) (1.18.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mltu) (3.7.1)\n",
      "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from qqdm==0.0.7->mltu) (2.4.0)\n",
      "Requirement already satisfied: jupyter in /usr/local/lib/python3.10/dist-packages (from qqdm==0.0.7->mltu) (1.0.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.15.0->mltu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.15.0->mltu) (24.3.25)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.15.0->mltu) (24.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.15.0->mltu) (3.20.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.15.0->mltu) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mltu) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mltu) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mltu) (1.16.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.15.0->mltu) (10.0)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm==0.0.7->mltu) (6.5.5)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm==0.0.7->mltu) (5.5.2)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm==0.0.7->mltu) (6.1.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm==0.0.7->mltu) (6.5.4)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm==0.0.7->mltu) (5.5.6)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm==0.0.7->mltu) (7.7.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.15.0->mltu) (1.3.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (0.2.0)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (7.34.0)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (5.7.1)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (6.3.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->qqdm==0.0.7->mltu) (3.6.8)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->qqdm==0.0.7->mltu) (3.0.11)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->qqdm==0.0.7->mltu) (3.0.47)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->qqdm==0.0.7->mltu) (2.16.1)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (4.9.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (4.12.3)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (0.7.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (0.4)\n",
      "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (3.1.4)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (5.7.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (0.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (2.1.5)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (0.8.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (1.3.0)\n",
      "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (24.0.1)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (23.1.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (1.6.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (0.18.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (0.20.0)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (1.1.0)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter->qqdm==0.0.7->mltu) (2.4.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (71.0.4)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (0.19.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (0.7.5)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter->qqdm==0.0.7->mltu) (4.2.2)\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter->qqdm==0.0.7->mltu) (0.2.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->qqdm==0.0.7->mltu) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->qqdm==0.0.7->mltu) (4.23.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->qqdm==0.0.7->mltu) (0.2.13)\n",
      "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter->qqdm==0.0.7->mltu) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter->qqdm==0.0.7->mltu) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter->qqdm==0.0.7->mltu) (2.5)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter->qqdm==0.0.7->mltu) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->qqdm==0.0.7->mltu) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->qqdm==0.0.7->mltu) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->qqdm==0.0.7->mltu) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->qqdm==0.0.7->mltu) (0.20.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter->qqdm==0.0.7->mltu) (1.24.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->qqdm==0.0.7->mltu) (1.17.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->qqdm==0.0.7->mltu) (2.22)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter->qqdm==0.0.7->mltu) (3.7.1)\n",
      "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter->qqdm==0.0.7->mltu) (1.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter->qqdm==0.0.7->mltu) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter->qqdm==0.0.7->mltu) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter->qqdm==0.0.7->mltu) (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "pip install mltu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "tBBaeXN5jvXT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643302381,
     "user_tz": -210,
     "elapsed": 8,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "outputId": "d0bba3e4-a90a-4df2-d866-96765aa0fda1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:DataProvider:Skipping Dataset validation...\n",
      "INFO:DataProvider:Skipping Dataset validation...\n"
     ]
    }
   ],
   "source": [
    "from mltu.tensorflow.dataProvider import DataProvider\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_inputs(data_batch, label_batch):\n",
    "    encoder_input = np.zeros((len(data_batch), tokenizer.max_length)).astype(np.int64)\n",
    "    decoder_input = np.zeros((len(label_batch), detokenizer.max_length)).astype(np.int64)\n",
    "    decoder_output = np.zeros((len(label_batch), detokenizer.max_length)).astype(np.int64)\n",
    "\n",
    "    data_batch_tokens = tokenizer.texts_to_sequences(data_batch)\n",
    "    label_batch_tokens = detokenizer.texts_to_sequences(label_batch)\n",
    "\n",
    "    for index, (data, label) in enumerate(zip(data_batch_tokens, label_batch_tokens)):\n",
    "        encoder_input[index][:len(data)] = data\n",
    "        decoder_input[index][:len(label)-1] = label[:-1] # Drop the [END] tokens\n",
    "        decoder_output[index][:len(label)-1] = label[1:] # Drop the [START] tokens\n",
    "\n",
    "    return (encoder_input, decoder_input), decoder_output\n",
    "\n",
    "train_dataProvider = DataProvider(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    batch_postprocessors=[preprocess_inputs],\n",
    "    use_cache=True\n",
    "    )\n",
    "\n",
    "val_dataProvider = DataProvider(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    batch_postprocessors=[preprocess_inputs],\n",
    "    use_cache=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "J93V0ppExe6J",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643302381,
     "user_tz": -210,
     "elapsed": 6,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "outputId": "75a0e703-fb9a-40e3-ddd6-809932097b39",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['وسايلتو جمع کن باشه.', 'يادمه يه گاو اونجا بود', 'و رييس سازمان \"ياماگاتو\" هم بين کشته ها هستن', 'اِی #@%$* توش، غارِ یخـی؟']\n",
      "['<start>pack your stuff.', '<start>i remember the cow that stayed over there.', '<start>_', '<start>oh']\n",
      "['pack your stuff.<eos>', 'i remember the cow that stayed over there.<eos>', '_<eos>', 'oh<eos>']\n"
     ]
    }
   ],
   "source": [
    "for data_batch in train_dataProvider:\n",
    "    (encoder_inputs, decoder_inputs), decoder_outputs = data_batch\n",
    "\n",
    "    encoder_inputs_str = tokenizer.detokenize(encoder_inputs)\n",
    "    decoder_inputs_str = detokenizer.detokenize(decoder_inputs, remove_start_end=False)\n",
    "    decoder_outputs_str = detokenizer.detokenize(decoder_outputs, remove_start_end=False)\n",
    "    print(encoder_inputs_str)\n",
    "    print(decoder_inputs_str)\n",
    "    print(decoder_outputs_str)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "v_wMwgWayeUa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643302382,
     "user_tz": -210,
     "elapsed": 5,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "try: [tf.config.experimental.set_memory_growth(gpu, True) for gpu in tf.config.experimental.list_physical_devices(\"GPU\")]\n",
    "except: pass\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from mltu.tensorflow.callbacks import Model2onnx, WarmupCosineDecay\n",
    "\n",
    "from mltu.tensorflow.dataProvider import DataProvider\n",
    "from mltu.tokenizers import CustomTokenizer\n",
    "\n",
    "from mltu.tensorflow.transformer.utils import MaskedAccuracy, MaskedLoss\n",
    "from mltu.tensorflow.transformer.callbacks import EncDecSplitCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "T9IKYCHbyqtJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643302382,
     "user_tz": -210,
     "elapsed": 5,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from mltu.tensorflow.transformer.layers import Encoder, Decoder\n",
    "\n",
    "def Transformer(\n",
    "    input_vocab_size: int,\n",
    "    target_vocab_size: int,\n",
    "    encoder_input_size: int = None,\n",
    "    decoder_input_size: int = None,\n",
    "    num_layers: int=6,\n",
    "    d_model: int=512,\n",
    "    num_heads: int=8,\n",
    "    dff: int=2048,\n",
    "    dropout_rate: float=0.1,\n",
    "    ) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    A custom TensorFlow model that implements the Transformer architecture.\n",
    "\n",
    "    Args:\n",
    "        input_vocab_size (int): The size of the input vocabulary.\n",
    "        target_vocab_size (int): The size of the target vocabulary.\n",
    "        encoder_input_size (int): The size of the encoder input sequence.\n",
    "        decoder_input_size (int): The size of the decoder input sequence.\n",
    "        num_layers (int): The number of layers in the encoder and decoder.\n",
    "        d_model (int): The dimensionality of the model.\n",
    "        num_heads (int): The number of heads in the multi-head attention layer.\n",
    "        dff (int): The dimensionality of the feed-forward layer.\n",
    "        dropout_rate (float): The dropout rate.\n",
    "\n",
    "    Returns:\n",
    "        A TensorFlow Keras model.\n",
    "    \"\"\"\n",
    "    inputs = [\n",
    "        tf.keras.layers.Input(shape=(encoder_input_size,), dtype=tf.int64),\n",
    "        tf.keras.layers.Input(shape=(decoder_input_size,), dtype=tf.int64)\n",
    "        ]\n",
    "\n",
    "    encoder_input, decoder_input = inputs\n",
    "\n",
    "    encoder = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, vocab_size=input_vocab_size, dropout_rate=dropout_rate)(encoder_input)\n",
    "    decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, vocab_size=target_vocab_size, dropout_rate=dropout_rate)(decoder_input, encoder)\n",
    "\n",
    "    output = tf.keras.layers.Dense(target_vocab_size)(decoder)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "CsUE236fyvSR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643302383,
     "user_tz": -210,
     "elapsed": 5,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#configs.py\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from mltu.configs import BaseModelConfigs\n",
    "\n",
    "\n",
    "class ModelConfigs(BaseModelConfigs):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_path = '/content/drive/MyDrive/Colab Notebooks/Models'\n",
    "        self.num_layers = 1\n",
    "        self.d_model = 128\n",
    "        self.num_heads = 8\n",
    "        self.dff = 512\n",
    "        self.dropout_rate = 0.1\n",
    "        self.batch_size = 64\n",
    "        self.train_epochs = 1\n",
    "        # CustomSchedule parameters\n",
    "        self.init_lr = 0.00001\n",
    "        self.lr_after_warmup = 0.0005\n",
    "        self.final_lr = 0.0001\n",
    "        self.warmup_epochs = 1\n",
    "        self.decay_epochs = 9\n",
    "\n",
    "        # self.num_layers = 4\n",
    "        # self.d_model = 128\n",
    "        # self.num_heads = 8\n",
    "        # self.dff = 512\n",
    "        # self.dropout_rate = 0.1\n",
    "        # self.batch_size = 16\n",
    "        # self.train_epochs = 50\n",
    "        # # CustomSchedule parameters\n",
    "        # self.init_lr = 0.00001\n",
    "        # self.lr_after_warmup = 0.0005\n",
    "        # self.final_lr = 0.0001\n",
    "        # self.warmup_epochs = 2\n",
    "        # self.decay_epochs = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "GqO0bu2Oyxop",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643302383,
     "user_tz": -210,
     "elapsed": 5,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#train.py\n",
    "\n",
    "configs = ModelConfigs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "gBoVYZG_y_lE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643307509,
     "user_tz": -210,
     "elapsed": 5131,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "en_training_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-train.en\"\n",
    "en_validation_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-dev.en\"\n",
    "fa_training_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-train.fa\"\n",
    "fa_validation_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-dev.fa\"\n",
    "\n",
    "def read_files(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        en_train_dataset = f.read().split(\"\\n\")[:-1]\n",
    "    return en_train_dataset\n",
    "\n",
    "en_training_data = read_files(en_training_data_path)\n",
    "en_validation_data = read_files(en_validation_data_path)\n",
    "fa_training_data = read_files(fa_training_data_path)\n",
    "fa_validation_data = read_files(fa_validation_data_path)\n",
    "\n",
    "# Consider only sentences with length <= 500\n",
    "max_lenght = 500\n",
    "train_dataset = [[fa_sentence, en_sentence] for fa_sentence, en_sentence in zip(fa_training_data, en_training_data) if len(fa_sentence) <= max_lenght and len(en_sentence) <= max_lenght]\n",
    "val_dataset = [[fa_sentence, en_sentence] for fa_sentence, en_sentence in zip(fa_validation_data, en_validation_data) if len(fa_sentence) <= max_lenght and len(en_sentence) <= max_lenght]\n",
    "fa_training_data, en_training_data = zip(*train_dataset)\n",
    "fa_validation_data, en_validation_data = zip(*val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "RIQTNsPkzQBh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643319964,
     "user_tz": -210,
     "elapsed": 12468,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "outputId": "b1df1b33-1c7f-489d-bf34-e94f9ff050a0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Fitting tokenizer: 100%|██████████| 996371/996371 [00:06<00:00, 144890.01it/s]\n",
      "Fitting tokenizer: 100%|██████████| 996371/996371 [00:04<00:00, 202888.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# prepare spanish tokenizer, this is the input language\n",
    "tokenizer = CustomTokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(fa_training_data)\n",
    "tokenizer.save(configs.model_path + \"/tokenizer.json\")\n",
    "\n",
    "# prepare english tokenizer, this is the output language\n",
    "detokenizer = CustomTokenizer(char_level=True)\n",
    "detokenizer.fit_on_texts(en_training_data)\n",
    "detokenizer.save(configs.model_path + \"/detokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ucEwxjVXzUu4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643319964,
     "user_tz": -210,
     "elapsed": 18,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_inputs(data_batch, label_batch):\n",
    "    encoder_input = np.zeros((len(data_batch), tokenizer.max_length)).astype(np.int64)\n",
    "    decoder_input = np.zeros((len(label_batch), detokenizer.max_length)).astype(np.int64)\n",
    "    decoder_output = np.zeros((len(label_batch), detokenizer.max_length)).astype(np.int64)\n",
    "\n",
    "    data_batch_tokens = tokenizer.texts_to_sequences(data_batch)\n",
    "    label_batch_tokens = detokenizer.texts_to_sequences(label_batch)\n",
    "\n",
    "    for index, (data, label) in enumerate(zip(data_batch_tokens, label_batch_tokens)):\n",
    "        encoder_input[index][:len(data)] = data\n",
    "        decoder_input[index][:len(label)-1] = label[:-1] # Drop the [END] tokens\n",
    "        decoder_output[index][:len(label)-1] = label[1:] # Drop the [START] tokens\n",
    "\n",
    "    return (encoder_input, decoder_input), decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "Fk30YkY4zYZ4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643319964,
     "user_tz": -210,
     "elapsed": 17,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "outputId": "2b842720-e11d-49bf-8d1d-a74263a49350",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:DataProvider:Skipping Dataset validation...\n",
      "INFO:DataProvider:Skipping Dataset validation...\n"
     ]
    }
   ],
   "source": [
    "# Create Training Data Provider\n",
    "train_dataProvider = DataProvider(\n",
    "    train_dataset,\n",
    "    batch_size=configs.batch_size,\n",
    "    batch_postprocessors=[preprocess_inputs],\n",
    "    use_cache=True,\n",
    "    )\n",
    "\n",
    "# Create Validation Data Provider\n",
    "val_dataProvider = DataProvider(\n",
    "    val_dataset,\n",
    "    batch_size=configs.batch_size,\n",
    "    batch_postprocessors=[preprocess_inputs],\n",
    "    use_cache=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Ha6eZr2vza9o",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643320913,
     "user_tz": -210,
     "elapsed": 964,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "outputId": "0b5c6cef-2cff-461c-9531-32e9ada326c3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'global_self_attention_3' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'sequential_6' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'feed_forward_6' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'encoder_layer_3' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'causal_self_attention_3' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'cross_attention_3' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'sequential_7' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'feed_forward_7' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'decoder_layer_3' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional_10\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_10\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)             \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m       Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to          \u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_10            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m502\u001B[0m)            │              \u001B[38;5;34m0\u001B[0m │ -                      │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ input_layer_11            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m502\u001B[0m)            │              \u001B[38;5;34m0\u001B[0m │ -                      │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ encoder_2 (\u001B[38;5;33mEncoder\u001B[0m)       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m502\u001B[0m, \u001B[38;5;34m128\u001B[0m)       │        \u001B[38;5;34m720,640\u001B[0m │ input_layer_10[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder_2 (\u001B[38;5;33mDecoder\u001B[0m)       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m502\u001B[0m, \u001B[38;5;34m128\u001B[0m)       │      \u001B[38;5;34m1,251,712\u001B[0m │ input_layer_11[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],  │\n",
       "│                           │                        │                │ encoder_2[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_18 (\u001B[38;5;33mDense\u001B[0m)          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m502\u001B[0m, \u001B[38;5;34m502\u001B[0m)       │         \u001B[38;5;34m64,758\u001B[0m │ decoder_2[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]        │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_10            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">502</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ input_layer_11            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">502</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ encoder_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">502</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">720,640</span> │ input_layer_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">502</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,251,712</span> │ input_layer_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                           │                        │                │ encoder_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">502</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">502</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64,758</span> │ decoder_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m2,037,110\u001B[0m (7.77 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,037,110</span> (7.77 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m2,037,110\u001B[0m (7.77 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,037,110</span> (7.77 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Create TensorFlow Transformer Model\n",
    "transformer = Transformer(\n",
    "    num_layers=configs.num_layers,\n",
    "    d_model=configs.d_model,\n",
    "    num_heads=configs.num_heads,\n",
    "    dff=configs.dff,\n",
    "    input_vocab_size=len(tokenizer)+1,\n",
    "    target_vocab_size=len(detokenizer)+1,\n",
    "    dropout_rate=configs.dropout_rate,\n",
    "    encoder_input_size=tokenizer.max_length,\n",
    "    decoder_input_size=detokenizer.max_length\n",
    "    )\n",
    "\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "axvnqCYlzlmA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643320913,
     "user_tz": -210,
     "elapsed": 6,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=configs.init_lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# Compile the model\n",
    "transformer.compile(\n",
    "    loss=MaskedLoss(),\n",
    "    optimizer=optimizer,\n",
    "    metrics=[MaskedAccuracy()],\n",
    "    run_eagerly=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "0eCXjH4s0xef",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643322888,
     "user_tz": -210,
     "elapsed": 1981,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "outputId": "efac9ba1-13a8-4e6e-9917-e84f4dbf0864",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tf2onnx in /usr/local/lib/python3.10/dist-packages (1.16.1)\n",
      "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.26.4)\n",
      "Requirement already satisfied: onnx>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.16.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (2.32.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (24.3.25)\n",
      "Requirement already satisfied: protobuf~=3.20 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (3.20.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "pip install tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "7YQwNST3zo5H",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643322889,
     "user_tz": -210,
     "elapsed": 5,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "warmupCosineDecay = WarmupCosineDecay(\n",
    "    lr_after_warmup=configs.lr_after_warmup,\n",
    "    final_lr=configs.final_lr,\n",
    "    warmup_epochs=configs.warmup_epochs,\n",
    "    decay_epochs=configs.decay_epochs,\n",
    "    initial_lr=configs.init_lr,\n",
    "    )\n",
    "earlystopper = EarlyStopping(monitor=\"val_masked_accuracy\", patience=5, verbose=1, mode=\"max\")\n",
    "checkpoint = ModelCheckpoint(f\"{configs.model_path}/model.keras\", monitor=\"val_masked_accuracy\", verbose=1, save_best_only=True, mode=\"max\", save_weights_only=False)\n",
    "tb_callback = TensorBoard(f\"{configs.model_path}/logs\")\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor=\"val_masked_accuracy\", factor=0.9, min_delta=1e-10, patience=2, verbose=1, mode=\"max\")\n",
    "model2onnx = Model2onnx(f\"{configs.model_path}/model.keras\", metadata={\"tokenizer\": tokenizer.dict(), \"detokenizer\": detokenizer.dict()}, save_on_epoch_end=False)\n",
    "encDecSplitCallback = EncDecSplitCallback(configs.model_path, encoder_metadata={\"tokenizer\": tokenizer.dict()}, decoder_metadata={\"detokenizer\": detokenizer.dict()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "TsWquw5QBQI4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643322889,
     "user_tz": -210,
     "elapsed": 4,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a patched version of on_epoch_begin\n",
    "def patched_on_epoch_begin(self, epoch, logs=None):\n",
    "    if epoch < self.warmup_epochs:\n",
    "        # Apply warmup schedule\n",
    "        lr = self.initial_lr * (epoch + 1) / self.warmup_epochs\n",
    "    else:\n",
    "        # Apply cosine decay\n",
    "        progress = (epoch - self.warmup_epochs) / self.decay_epochs\n",
    "        lr = self.final_lr + 0.5 * (self.lr_after_warmup - self.final_lr) * (1 + tf.cos(tf.constant(progress) * 3.14159))\n",
    "\n",
    "    # Ensure the learning rate is set correctly\n",
    "    if hasattr(self.model.optimizer, 'lr'):\n",
    "        learning_rate_attr = self.model.optimizer.lr\n",
    "    else:\n",
    "        learning_rate_attr = self.model.optimizer.learning_rate\n",
    "\n",
    "    # Update learning rate if it's a tf.Variable or equivalent\n",
    "    if isinstance(learning_rate_attr, tf.Variable):\n",
    "        tf.keras.backend.set_value(learning_rate_attr, lr)\n",
    "    else:\n",
    "        # Create a new tf.Variable for learning rate if necessary\n",
    "        self.model.optimizer.learning_rate = tf.Variable(lr, dtype=tf.float32)\n",
    "\n",
    "    if self.verbose:\n",
    "        print(f\"Epoch {epoch+1}: Learning rate is {lr:.7f}.\")\n",
    "\n",
    "# Apply the patch to the callback\n",
    "WarmupCosineDecay.on_epoch_begin = patched_on_epoch_begin\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def patched_on_epoch_end(self, epoch: int, logs: dict=None):\n",
    "  logs = logs or {}\n",
    "\n",
    "  if hasattr(self.model.optimizer, 'lr'):\n",
    "    logs[\"lr\"] = self.model.optimizer.lr\n",
    "  else:\n",
    "    logs[\"lr\"] = self.model.optimizer.learning_rate\n",
    "  # Log the learning rate value\n",
    "  # logs[\"lr\"] = self.model.optimizer.lr\n",
    "\n",
    "  return logs\n",
    "\n",
    "WarmupCosineDecay.on_epoch_end = patched_on_epoch_end\n"
   ],
   "metadata": {
    "id": "TcQAmTavLbhb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723643322889,
     "user_tz": -210,
     "elapsed": 4,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 66,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XRQScGvr039u",
    "outputId": "d178cd2b-013c-4735-9e91-9ac05f7ea795",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723644164781,
     "user_tz": -210,
     "elapsed": 841896,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[1m15569/15569\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 51ms/step - loss: 2.9191 - masked_accuracy: 0.2709\n",
      "Epoch 1: val_masked_accuracy improved from -inf to 0.35777, saving model to /content/drive/MyDrive/Colab Notebooks/Models/model.keras\n",
      "\u001B[1m15569/15569\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m840s\u001B[0m 52ms/step - loss: 2.9191 - masked_accuracy: 0.2709 - val_loss: 2.1719 - val_masked_accuracy: 0.3578 - lr: 1.0000e-05 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'Functional' object has no attribute '_get_save_spec'\n",
      "Error parsing message with type 'onnx.ModelProto'\n",
      "[Errno 2] Unable to synchronously open file (unable to open file: name = '/content/drive/MyDrive/Colab Notebooks/Models/model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "transformer.fit(\n",
    "    train_dataProvider,\n",
    "    validation_data=val_dataProvider,\n",
    "    epochs=configs.train_epochs,\n",
    "    callbacks=[\n",
    "        warmupCosineDecay,\n",
    "        checkpoint,\n",
    "        tb_callback,\n",
    "        reduceLROnPlat,\n",
    "        model2onnx,\n",
    "        encDecSplitCallback\n",
    "        ]\n",
    "    )\n",
    "\n",
    "transformer.save(f\"/content/drive/MyDrive/Colab Notebooks/Models/model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install tf2onnx\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mzucsci4OsmI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1723644784760,
     "user_tz": -210,
     "elapsed": 3038,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "outputId": "b5196a47-ad70-4680-ba3e-855fb976124a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tf2onnx in /usr/local/lib/python3.10/dist-packages (1.16.1)\n",
      "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.26.4)\n",
      "Requirement already satisfied: onnx>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.16.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (2.32.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (24.3.25)\n",
      "Requirement already satisfied: protobuf~=3.20 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (3.20.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2024.7.4)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import tf2onnx\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "from mltu.tensorflow.transformer.layers import Encoder  # Import the custom layer or object\n",
    "\n",
    "with custom_object_scope({'Encoder': Encoder}):\n",
    "    keras_model = load_model(\"/content/drive/MyDrive/Colab Notebooks/Models/model.h5\")\n",
    "\n",
    "# Convert the Keras model to ONNX format\n",
    "spec = (tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype),)\n",
    "output_path = \"/content/drive/MyDrive/Colab Notebooks/Models/model.onnx\"\n",
    "model_proto, _ = tf2onnx.convert.from_keras(keras_model, input_signature=spec, opset=13)\n",
    "\n",
    "# Save the ONNX model\n",
    "with open(output_path, \"wb\") as f:\n",
    "    f.write(model_proto.SerializeToString())\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-f2E0JtxNpLy",
    "executionInfo": {
     "status": "error",
     "timestamp": 1723645244914,
     "user_tz": -210,
     "elapsed": 1450,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "outputId": "8d3fe482-3389-4b44-eeff-71e0c89e08f4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'encoder_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'decoder_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'global_self_attention_2' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'sequential_2' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'feed_forward_2' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'encoder_layer_2' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'causal_self_attention' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'cross_attention' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'sequential_3' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'feed_forward_3' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'decoder_layer' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "<class 'mltu.tensorflow.transformer.utils.MaskedLoss'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'mltu.tensorflow.transformer.utils', 'class_name': 'MaskedLoss', 'config': {'name': 'masked_loss_2', 'reduction': 'none'}, 'registered_name': 'MaskedLoss'}.\n\nException encountered: MaskedLoss.__init__() got an unexpected keyword argument 'name'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/serialization_lib.py\u001B[0m in \u001B[0;36mdeserialize_keras_object\u001B[0;34m(config, custom_objects, safe_mode, **kwargs)\u001B[0m\n\u001B[1;32m    717\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 718\u001B[0;31m             \u001B[0minstance\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_config\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minner_config\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    719\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/losses/loss.py\u001B[0m in \u001B[0;36mfrom_config\u001B[0;34m(cls, config)\u001B[0m\n\u001B[1;32m     70\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfrom_config\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 71\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     72\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: MaskedLoss.__init__() got an unexpected keyword argument 'name'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-e95a408e6797>\u001B[0m in \u001B[0;36m<cell line: 8>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mcustom_object_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'Encoder'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mEncoder\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m     \u001B[0mkeras_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"/content/drive/MyDrive/Colab Notebooks/Models/model.keras\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;31m# Convert the Keras model to ONNX format\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001B[0m in \u001B[0;36mload_model\u001B[0;34m(filepath, custom_objects, compile, safe_mode)\u001B[0m\n\u001B[1;32m    180\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    181\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mis_keras_zip\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mis_keras_dir\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 182\u001B[0;31m         return saving_lib.load_model(\n\u001B[0m\u001B[1;32m    183\u001B[0m             \u001B[0mfilepath\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    184\u001B[0m             \u001B[0mcustom_objects\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcustom_objects\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001B[0m in \u001B[0;36mload_model\u001B[0;34m(filepath, custom_objects, compile, safe_mode)\u001B[0m\n\u001B[1;32m    235\u001B[0m             )\n\u001B[1;32m    236\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"rb\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 237\u001B[0;31m             return _load_model_from_fileobj(\n\u001B[0m\u001B[1;32m    238\u001B[0m                 \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcustom_objects\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcompile\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msafe_mode\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    239\u001B[0m             )\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001B[0m in \u001B[0;36m_load_model_from_fileobj\u001B[0;34m(fileobj, custom_objects, compile, safe_mode)\u001B[0m\n\u001B[1;32m    312\u001B[0m             \u001B[0mconfig_json\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 314\u001B[0;31m         model = _model_from_config(\n\u001B[0m\u001B[1;32m    315\u001B[0m             \u001B[0mconfig_json\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcustom_objects\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcompile\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msafe_mode\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    316\u001B[0m         )\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001B[0m in \u001B[0;36m_model_from_config\u001B[0;34m(config_json, custom_objects, compile, safe_mode)\u001B[0m\n\u001B[1;32m    301\u001B[0m     \u001B[0;31m# Construct the model from the configuration file in the archive.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    302\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mObjectSharingScope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 303\u001B[0;31m         model = deserialize_keras_object(\n\u001B[0m\u001B[1;32m    304\u001B[0m             \u001B[0mconfig_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcustom_objects\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msafe_mode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msafe_mode\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    305\u001B[0m         )\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/serialization_lib.py\u001B[0m in \u001B[0;36mdeserialize_keras_object\u001B[0;34m(config, custom_objects, safe_mode, **kwargs)\u001B[0m\n\u001B[1;32m    732\u001B[0m         \u001B[0mcompile_config\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"compile_config\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    733\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcompile_config\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 734\u001B[0;31m             \u001B[0minstance\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompile_from_config\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcompile_config\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    735\u001B[0m             \u001B[0minstance\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompiled\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    736\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\u001B[0m in \u001B[0;36mcompile_from_config\u001B[0;34m(self, config)\u001B[0m\n\u001B[1;32m    900\u001B[0m             )\n\u001B[1;32m    901\u001B[0m             \u001B[0;32mreturn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 902\u001B[0;31m         \u001B[0mconfig\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mserialization_lib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdeserialize_keras_object\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    903\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    904\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"optimizer\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuilt\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/serialization_lib.py\u001B[0m in \u001B[0;36mdeserialize_keras_object\u001B[0;34m(config, custom_objects, safe_mode, **kwargs)\u001B[0m\n\u001B[1;32m    592\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    593\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;34m\"class_name\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mconfig\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;34m\"config\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 594\u001B[0;31m         return {\n\u001B[0m\u001B[1;32m    595\u001B[0m             key: deserialize_keras_object(\n\u001B[1;32m    596\u001B[0m                 \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcustom_objects\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcustom_objects\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msafe_mode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msafe_mode\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/serialization_lib.py\u001B[0m in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    593\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;34m\"class_name\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mconfig\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;34m\"config\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    594\u001B[0m         return {\n\u001B[0;32m--> 595\u001B[0;31m             key: deserialize_keras_object(\n\u001B[0m\u001B[1;32m    596\u001B[0m                 \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcustom_objects\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcustom_objects\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msafe_mode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msafe_mode\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    597\u001B[0m             )\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/serialization_lib.py\u001B[0m in \u001B[0;36mdeserialize_keras_object\u001B[0;34m(config, custom_objects, safe_mode, **kwargs)\u001B[0m\n\u001B[1;32m    718\u001B[0m             \u001B[0minstance\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_config\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minner_config\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    719\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 720\u001B[0;31m             raise TypeError(\n\u001B[0m\u001B[1;32m    721\u001B[0m                 \u001B[0;34mf\"{cls} could not be deserialized properly. Please\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    722\u001B[0m                 \u001B[0;34m\" ensure that components that are Python object\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: <class 'mltu.tensorflow.transformer.utils.MaskedLoss'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'mltu.tensorflow.transformer.utils', 'class_name': 'MaskedLoss', 'config': {'name': 'masked_loss_2', 'reduction': 'none'}, 'registered_name': 'MaskedLoss'}.\n\nException encountered: MaskedLoss.__init__() got an unexpected keyword argument 'name'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "kqhIHUAg_vfh",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1723644653904,
     "user_tz": -210,
     "elapsed": 524,
     "user": {
      "displayName": "Ayan Ayan",
      "userId": "01227115496038093893"
     }
    },
    "outputId": "eb8c4c75-07fe-43d7-f043-521e97f4e013",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "InvalidProtobuf",
     "evalue": "[ONNXRuntimeError] : 7 : INVALID_PROTOBUF : Load model from /content/drive/MyDrive/Colab Notebooks/Models/model.h5 failed:Protobuf parsing failed.",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidProtobuf\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-77-6970aa6259bc>\u001B[0m in \u001B[0;36m<cell line: 34>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[0mval_examples\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfa_sentence\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0men_sentence\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mfa_sentence\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0men_sentence\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfa_validation_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0men_validation_data\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfa_sentence\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mmax_lenght\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0men_sentence\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mmax_lenght\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 34\u001B[0;31m \u001B[0mtranslator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPtEnTranslator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"/content/drive/MyDrive/Colab Notebooks/Models/model.h5\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     35\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0mval_dataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-77-6970aa6259bc>\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mPtEnTranslator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mOnnxInferenceModel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnew_inputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_inputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/mltu/inferenceModel.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, model_path, force_cpu, default_model_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m     53\u001B[0m         \u001B[0mproviders\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"CUDAExecutionProvider\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"CPUExecutionProvider\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mort\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_device\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"GPU\"\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mforce_cpu\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"CPUExecutionProvider\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 55\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mort\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mInferenceSession\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mproviders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mproviders\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetadata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001B[0m\n\u001B[1;32m    417\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    418\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 419\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_inference_session\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mproviders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprovider_options\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdisabled_optimizers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    420\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mValueError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    421\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_enable_fallback\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001B[0m in \u001B[0;36m_create_inference_session\u001B[0;34m(self, providers, provider_options, disabled_optimizers)\u001B[0m\n\u001B[1;32m    470\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    471\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_model_path\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 472\u001B[0;31m             \u001B[0msess\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mC\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mInferenceSession\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msession_options\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_model_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_read_config_from_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    473\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    474\u001B[0m             \u001B[0msess\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mC\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mInferenceSession\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msession_options\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_model_bytes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_read_config_from_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mInvalidProtobuf\u001B[0m: [ONNXRuntimeError] : 7 : INVALID_PROTOBUF : Load model from /content/drive/MyDrive/Colab Notebooks/Models/model.h5 failed:Protobuf parsing failed."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from mltu.tokenizers import CustomTokenizer\n",
    "from mltu.inferenceModel import OnnxInferenceModel\n",
    "\n",
    "class PtEnTranslator(OnnxInferenceModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.new_inputs = self.model.get_inputs()\n",
    "        self.tokenizer = CustomTokenizer.load(self.metadata[\"tokenizer\"])\n",
    "        self.detokenizer = CustomTokenizer.load(self.metadata[\"detokenizer\"])\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        start = time.time()\n",
    "        tokenized_sentence = self.tokenizer.texts_to_sequences([sentence])[0]\n",
    "        encoder_input = np.pad(tokenized_sentence, (0, self.tokenizer.max_length - len(tokenized_sentence)), constant_values=0).astype(np.int64)\n",
    "\n",
    "        tokenized_results = [self.detokenizer.start_token_index]\n",
    "        for index in range(self.detokenizer.max_length - 1):\n",
    "            decoder_input = np.pad(tokenized_results, (0, self.detokenizer.max_length - len(tokenized_results)), constant_values=0).astype(np.int64)\n",
    "            input_dict = {\n",
    "                self.model._inputs_meta[0].name: np.expand_dims(encoder_input, axis=0),\n",
    "                self.model._inputs_meta[1].name: np.expand_dims(decoder_input, axis=0),\n",
    "            }\n",
    "            preds = self.model.run(None, input_dict)[0] # preds shape (1, 206, 29110)\n",
    "            pred_results = np.argmax(preds, axis=2)\n",
    "            tokenized_results.append(pred_results[0][index])\n",
    "\n",
    "            if tokenized_results[-1] == self.detokenizer.end_token_index:\n",
    "                break\n",
    "\n",
    "        results = self.detokenizer.detokenize([tokenized_results])\n",
    "        return results[0], time.time() - start\n",
    "\n",
    "def read_files(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        en_train_dataset = f.read().split(\"\\n\")[:-1]\n",
    "    return en_train_dataset\n",
    "\n",
    "# Path to dataset\n",
    "en_validation_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-dev.en\"\n",
    "fa_validation_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-dev.fa\"\n",
    "\n",
    "en_validation_data = read_files(en_validation_data_path)\n",
    "fa_validation_data = read_files(fa_validation_data_path)\n",
    "\n",
    "# Consider only sentences with length <= 500\n",
    "max_lenght = 500\n",
    "val_examples = [[fa_sentence, en_sentence] for fa_sentence, en_sentence in zip(fa_validation_data, en_validation_data) if len(fa_sentence) <= max_lenght and len(en_sentence) <= max_lenght]\n",
    "\n",
    "translator = PtEnTranslator(\"/content/drive/MyDrive/Colab Notebooks/Models/model.h5\")\n",
    "\n",
    "val_dataset = []\n",
    "for fa, en in val_examples:\n",
    "    results, duration = translator.predict(fa)\n",
    "    print(\"Farsi:     \", fa.lower())\n",
    "    print(\"English:     \", en.lower())\n",
    "    print(\"English pred:\", results)\n",
    "    print(duration)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}