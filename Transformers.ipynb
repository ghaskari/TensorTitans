{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OeA8fMn9hrYI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8715d25f-8ed4-4b81-b5d7-9c7a824fe499",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install mltu\n",
    "!pip install keras==2.11.0\n",
    "!pip install tensorflow==2.11.0\n",
    "!pip install tf2onnx==1.14.0\n",
    "!pip install onnx==1.12.0\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWJ3uDVxDTM_",
    "outputId": "cdd490da-3562-4450-df0c-031efdd27bab",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: mltu in /usr/local/lib/python3.10/dist-packages (1.2.5)\n",
      "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.10/dist-packages (from mltu) (6.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mltu) (4.66.5)\n",
      "Requirement already satisfied: qqdm==0.0.7 in /usr/local/lib/python3.10/dist-packages (from mltu) (0.0.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mltu) (2.1.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mltu) (1.26.4)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from mltu) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from mltu) (9.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from mltu) (1.19.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mltu) (3.7.1)\n",
      "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from qqdm==0.0.7->mltu) (2.4.0)\n",
      "Requirement already satisfied: jupyter in /usr/local/lib/python3.10/dist-packages (from qqdm==0.0.7->mltu) (1.0.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.15.0->mltu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.15.0->mltu) (2.0.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.15.0->mltu) (24.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.15.0->mltu) (3.19.6)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.15.0->mltu) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mltu) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mltu) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mltu) (1.16.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.15.0->mltu) (10.0)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm==0.0.7->mltu) (6.5.5)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm==0.0.7->mltu) (5.5.2)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm==0.0.7->mltu) (6.1.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm==0.0.7->mltu) (6.5.4)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm==0.0.7->mltu) (5.5.6)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter->qqdm==0.0.7->mltu) (7.7.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.15.0->mltu) (1.3.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (0.2.0)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (7.34.0)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (5.7.1)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (6.3.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->qqdm==0.0.7->mltu) (3.6.8)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->qqdm==0.0.7->mltu) (3.0.11)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->qqdm==0.0.7->mltu) (3.0.47)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->qqdm==0.0.7->mltu) (2.16.1)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (4.9.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (4.12.3)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (0.7.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (0.4)\n",
      "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (3.1.4)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (5.7.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (0.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (2.1.5)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (0.8.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (1.3.0)\n",
      "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (24.0.1)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (23.1.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (1.6.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (0.18.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (0.20.0)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (1.1.0)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter->qqdm==0.0.7->mltu) (2.4.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (71.0.4)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (0.19.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (0.7.5)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter->qqdm==0.0.7->mltu) (4.2.2)\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter->qqdm==0.0.7->mltu) (0.2.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->qqdm==0.0.7->mltu) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->qqdm==0.0.7->mltu) (4.23.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->qqdm==0.0.7->mltu) (0.2.13)\n",
      "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter->qqdm==0.0.7->mltu) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter->qqdm==0.0.7->mltu) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter->qqdm==0.0.7->mltu) (2.5)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter->qqdm==0.0.7->mltu) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter->qqdm==0.0.7->mltu) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->qqdm==0.0.7->mltu) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->qqdm==0.0.7->mltu) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->qqdm==0.0.7->mltu) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->qqdm==0.0.7->mltu) (0.20.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter->qqdm==0.0.7->mltu) (1.24.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->qqdm==0.0.7->mltu) (1.17.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->qqdm==0.0.7->mltu) (2.22)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter->qqdm==0.0.7->mltu) (3.7.1)\n",
      "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter->qqdm==0.0.7->mltu) (1.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter->qqdm==0.0.7->mltu) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter->qqdm==0.0.7->mltu) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter->qqdm==0.0.7->mltu) (1.2.2)\n",
      "Requirement already satisfied: keras==2.11.0 in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow==2.11.0 in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.0.7)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.64.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.11.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (24.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.19.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (71.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
      "Requirement already satisfied: tf2onnx==1.14.0 in /usr/local/lib/python3.10/dist-packages (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx==1.14.0) (1.26.4)\n",
      "Requirement already satisfied: onnx>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx==1.14.0) (1.12.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tf2onnx==1.14.0) (2.32.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx==1.14.0) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tf2onnx==1.14.0) (2.0.7)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.4.1->tf2onnx==1.14.0) (3.19.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.4.1->tf2onnx==1.14.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx==1.14.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx==1.14.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx==1.14.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx==1.14.0) (2024.7.4)\n",
      "Requirement already satisfied: onnx==1.12.0 in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from onnx==1.12.0) (1.26.4)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from onnx==1.12.0) (3.19.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx==1.12.0) (4.12.2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#config.py\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from mltu.configs import BaseModelConfigs\n",
    "\n",
    "\n",
    "class ModelConfigs(BaseModelConfigs):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_path = os.path.join(\n",
    "            \"/content/drive/MyDrive/Colab Notebooks/Models\",\n",
    "            datetime.strftime(datetime.now(), \"%Y%m%d%H%M\"),\n",
    "        )\n",
    "        # self.num_layers = 4\n",
    "        # self.d_model = 128\n",
    "        # self.num_heads = 8\n",
    "        # self.dff = 512\n",
    "        # self.dropout_rate = 0.1\n",
    "        # self.batch_size = 16\n",
    "        # self.train_epochs = 50\n",
    "        # # CustomSchedule parameters\n",
    "        # self.init_lr = 0.00001\n",
    "        # self.lr_after_warmup = 0.0005\n",
    "        # self.final_lr = 0.0001\n",
    "        # self.warmup_epochs = 2\n",
    "        # self.decay_epochs = 18\n",
    "        # Reduce the model to the simplest possible configuration\n",
    "        self.num_layers = 1  # Minimum number of layers\n",
    "        self.d_model = 16  # Very small model dimensionality\n",
    "        self.num_heads = 1  # Only 1 attention head\n",
    "        self.dff = 32  # Very small feed-forward network\n",
    "        self.dropout_rate = 0.1  # Small dropout to keep things simple\n",
    "\n",
    "        # Minimize batch size for fast processing\n",
    "        self.batch_size = 2  # Tiny batch size for quick iteration\n",
    "\n",
    "        # Train for only 1 epoch\n",
    "        self.train_epochs = 1  # Just 1 epoch\n",
    "\n",
    "        # Learning rate setup for quick processing\n",
    "        self.init_lr = 0.01  # High learning rate to ensure quick changes, not crucial here\n",
    "        self.lr_after_warmup = 0.01\n",
    "        self.final_lr = 0.01\n",
    "        self.warmup_epochs = 0  # Skip warmup entirely\n",
    "        self.decay_epochs = 0  # No decay, since we're not really training\n",
    "\n",
    "        # Optionally, you can reduce the dataset size used in this single epoch\n",
    "        # This will be handled by your data provider and how you split your dataset\n"
   ],
   "metadata": {
    "id": "s4bsoW73B5_R",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#model.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from mltu.tensorflow.transformer.layers import Encoder, Decoder\n",
    "\n",
    "def Transformer(\n",
    "    input_vocab_size: int,\n",
    "    target_vocab_size: int,\n",
    "    encoder_input_size: int = None,\n",
    "    decoder_input_size: int = None,\n",
    "    num_layers: int=6,\n",
    "    d_model: int=512,\n",
    "    num_heads: int=8,\n",
    "    dff: int=2048,\n",
    "    dropout_rate: float=0.1,\n",
    "    ) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    A custom TensorFlow model that implements the Transformer architecture.\n",
    "\n",
    "    Args:\n",
    "        input_vocab_size (int): The size of the input vocabulary.\n",
    "        target_vocab_size (int): The size of the target vocabulary.\n",
    "        encoder_input_size (int): The size of the encoder input sequence.\n",
    "        decoder_input_size (int): The size of the decoder input sequence.\n",
    "        num_layers (int): The number of layers in the encoder and decoder.\n",
    "        d_model (int): The dimensionality of the model.\n",
    "        num_heads (int): The number of heads in the multi-head attention layer.\n",
    "        dff (int): The dimensionality of the feed-forward layer.\n",
    "        dropout_rate (float): The dropout rate.\n",
    "\n",
    "    Returns:\n",
    "        A TensorFlow Keras model.\n",
    "    \"\"\"\n",
    "    inputs = [\n",
    "        tf.keras.layers.Input(shape=(encoder_input_size,), dtype=tf.int64),\n",
    "        tf.keras.layers.Input(shape=(decoder_input_size,), dtype=tf.int64)\n",
    "        ]\n",
    "\n",
    "    encoder_input, decoder_input = inputs\n",
    "\n",
    "    encoder = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, vocab_size=input_vocab_size, dropout_rate=dropout_rate)(encoder_input)\n",
    "    decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, vocab_size=target_vocab_size, dropout_rate=dropout_rate)(decoder_input, encoder)\n",
    "\n",
    "    output = tf.keras.layers.Dense(target_vocab_size)(decoder)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=output)"
   ],
   "metadata": {
    "id": "pmXgWxrYBxSM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#import keras\n",
    "#print(keras.version())"
   ],
   "metadata": {
    "id": "r-NQKQxDFDpK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#train.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "try: [tf.config.experimental.set_memory_growth(gpu, True) for gpu in tf.config.experimental.list_physical_devices(\"GPU\")]\n",
    "except: pass\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from mltu.tensorflow.callbacks import Model2onnx, WarmupCosineDecay\n",
    "\n",
    "from mltu.tensorflow.dataProvider import DataProvider\n",
    "from mltu.tokenizers import CustomTokenizer\n",
    "\n",
    "from mltu.tensorflow.transformer.utils import MaskedAccuracy, MaskedLoss\n",
    "from mltu.tensorflow.transformer.callbacks import EncDecSplitCallback\n",
    "\n",
    "#from model import Transformer\n",
    "#from configs import ModelConfigs\n",
    "\n",
    "configs = ModelConfigs()\n",
    "\n",
    "# Path to dataset\n",
    "en_training_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-train.en\"\n",
    "en_validation_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-dev.en\"\n",
    "es_training_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-train.fa\"\n",
    "es_validation_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-dev.fa\"\n",
    "\n",
    "def read_files(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        en_train_dataset = f.read().split(\"\\n\")[:-1]\n",
    "    return en_train_dataset\n",
    "\n",
    "en_training_data = read_files(en_training_data_path)\n",
    "en_validation_data = read_files(en_validation_data_path)\n",
    "es_training_data = read_files(es_training_data_path)\n",
    "es_validation_data = read_files(es_validation_data_path)\n",
    "\n",
    "# Consider only sentences with length <= 500\n",
    "max_lenght = 500\n",
    "train_dataset = [[es_sentence, en_sentence] for es_sentence, en_sentence in zip(es_training_data, en_training_data) if len(es_sentence) <= max_lenght and len(en_sentence) <= max_lenght]\n",
    "val_dataset = [[es_sentence, en_sentence] for es_sentence, en_sentence in zip(es_validation_data, en_validation_data) if len(es_sentence) <= max_lenght and len(en_sentence) <= max_lenght]\n",
    "es_training_data, en_training_data = zip(*train_dataset)\n",
    "es_validation_data, en_validation_data = zip(*val_dataset)\n",
    "\n",
    "# prepare spanish tokenizer, this is the input language\n",
    "tokenizer = CustomTokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(es_training_data)\n",
    "tokenizer.save(configs.model_path + \"/tokenizer.json\")\n",
    "\n",
    "# prepare english tokenizer, this is the output language\n",
    "detokenizer = CustomTokenizer(char_level=True)\n",
    "detokenizer.fit_on_texts(en_training_data)\n",
    "detokenizer.save(configs.model_path + \"/detokenizer.json\")\n",
    "\n",
    "\n",
    "def preprocess_inputs(data_batch, label_batch):\n",
    "    encoder_input = np.zeros((len(data_batch), tokenizer.max_length)).astype(np.int64)\n",
    "    decoder_input = np.zeros((len(label_batch), detokenizer.max_length)).astype(np.int64)\n",
    "    decoder_output = np.zeros((len(label_batch), detokenizer.max_length)).astype(np.int64)\n",
    "\n",
    "    data_batch_tokens = tokenizer.texts_to_sequences(data_batch)\n",
    "    label_batch_tokens = detokenizer.texts_to_sequences(label_batch)\n",
    "\n",
    "    for index, (data, label) in enumerate(zip(data_batch_tokens, label_batch_tokens)):\n",
    "        encoder_input[index][:len(data)] = data\n",
    "        decoder_input[index][:len(label)-1] = label[:-1] # Drop the [END] tokens\n",
    "        decoder_output[index][:len(label)-1] = label[1:] # Drop the [START] tokens\n",
    "\n",
    "    return (encoder_input, decoder_input), decoder_output\n",
    "\n",
    "# Create Training Data Provider\n",
    "train_dataProvider = DataProvider(\n",
    "    train_dataset,\n",
    "    batch_size=configs.batch_size,\n",
    "    batch_postprocessors=[preprocess_inputs],\n",
    "    use_cache=True,\n",
    "    )\n",
    "\n",
    "# Create Validation Data Provider\n",
    "val_dataProvider = DataProvider(\n",
    "    val_dataset,\n",
    "    batch_size=configs.batch_size,\n",
    "    batch_postprocessors=[preprocess_inputs],\n",
    "    use_cache=True,\n",
    "    )\n",
    "\n",
    "# Create TensorFlow Transformer Model\n",
    "transformer = Transformer(\n",
    "    num_layers=configs.num_layers,\n",
    "    d_model=configs.d_model,\n",
    "    num_heads=configs.num_heads,\n",
    "    dff=configs.dff,\n",
    "    input_vocab_size=len(tokenizer)+1,\n",
    "    target_vocab_size=len(detokenizer)+1,\n",
    "    dropout_rate=configs.dropout_rate,\n",
    "    encoder_input_size=tokenizer.max_length,\n",
    "    decoder_input_size=detokenizer.max_length\n",
    "    )\n",
    "\n",
    "transformer.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=configs.init_lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# Compile the model\n",
    "transformer.compile(\n",
    "    loss=MaskedLoss(),\n",
    "    optimizer=optimizer,\n",
    "    metrics=[MaskedAccuracy()],\n",
    "    run_eagerly=False\n",
    "    )\n",
    "\n",
    "# Define callbacks\n",
    "warmupCosineDecay = WarmupCosineDecay(\n",
    "    lr_after_warmup=configs.lr_after_warmup,\n",
    "    final_lr=configs.final_lr,\n",
    "    warmup_epochs=configs.warmup_epochs,\n",
    "    decay_epochs=configs.decay_epochs,\n",
    "    initial_lr=configs.init_lr,\n",
    "    )\n",
    "earlystopper = EarlyStopping(monitor=\"val_masked_accuracy\", patience=5, verbose=1, mode=\"max\")\n",
    "checkpoint = ModelCheckpoint(f\"{configs.model_path}/model.h5\", monitor=\"val_masked_accuracy\", verbose=1, save_best_only=True, mode=\"max\", save_weights_only=False)\n",
    "tb_callback = TensorBoard(f\"{configs.model_path}/logs\")\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor=\"val_masked_accuracy\", factor=0.9, min_delta=1e-10, patience=2, verbose=1, mode=\"max\")\n",
    "model2onnx = Model2onnx(f\"{configs.model_path}/model.h5\", metadata={\"tokenizer\": tokenizer.dict(), \"detokenizer\": detokenizer.dict()}, save_on_epoch_end=False)\n",
    "encDecSplitCallback = EncDecSplitCallback(configs.model_path, encoder_metadata={\"tokenizer\": tokenizer.dict()}, decoder_metadata={\"detokenizer\": detokenizer.dict()})\n",
    "\n",
    "configs.save()\n",
    "\n",
    "# Train the model\n",
    "transformer.fit(\n",
    "    train_dataProvider,\n",
    "    validation_data=val_dataProvider,\n",
    "    epochs=configs.train_epochs,\n",
    "    callbacks=[\n",
    "        earlystopper,\n",
    "        warmupCosineDecay,\n",
    "        checkpoint,\n",
    "        tb_callback,\n",
    "        reduceLROnPlat,\n",
    "        model2onnx,\n",
    "        encDecSplitCallback\n",
    "        ]\n",
    "    )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRXLCS-RBe00",
    "outputId": "655b6f2e-8158-441c-a2f2-dfdf7659bb68",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Fitting tokenizer: 100%|██████████| 996371/996371 [00:07<00:00, 138689.64it/s]\n",
      "Fitting tokenizer: 100%|██████████| 996371/996371 [00:05<00:00, 195205.84it/s]\n",
      "INFO:DataProvider:Skipping Dataset validation...\n",
      "INFO:DataProvider:Skipping Dataset validation...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 502)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 502)]        0           []                               \n",
      "                                                                                                  \n",
      " encoder_2 (Encoder)            (None, 502, 16)      9840        ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " decoder_2 (Decoder)            (None, 502, 16)      11376       ['input_6[0][0]',                \n",
      "                                                                  'encoder_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 502, 502)     8534        ['decoder_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,750\n",
      "Trainable params: 29,750\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "49/50 [============================>.] - ETA: 0s - loss: 3.6510 - masked_accuracy: 0.1552\n",
      "Epoch 1: val_masked_accuracy improved from -inf to 0.17110, saving model to /content/drive/MyDrive/Colab Notebooks/Models/202408181252/model.h5\n",
      "50/50 [==============================] - 8s 59ms/step - loss: 3.6411 - masked_accuracy: 0.1554 - val_loss: 3.1128 - val_masked_accuracy: 0.1711 - lr: 0.0100\n",
      "No such layer: encoder. Existing layers are: ['input_5', 'input_6', 'encoder_2', 'decoder_2', 'dense_14'].\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5d585df460>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#test.py\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from mltu.tokenizers import CustomTokenizer\n",
    "from mltu.inferenceModel import OnnxInferenceModel\n",
    "\n",
    "class PtEnTranslator(OnnxInferenceModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.new_inputs = self.model.get_inputs()\n",
    "        self.tokenizer = CustomTokenizer.load(self.metadata[\"tokenizer\"])\n",
    "        self.detokenizer = CustomTokenizer.load(self.metadata[\"detokenizer\"])\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        start = time.time()\n",
    "        tokenized_sentence = self.tokenizer.texts_to_sequences([sentence])[0]\n",
    "        encoder_input = np.pad(tokenized_sentence, (0, self.tokenizer.max_length - len(tokenized_sentence)), constant_values=0).astype(np.int64)\n",
    "\n",
    "        tokenized_results = [self.detokenizer.start_token_index]\n",
    "        for index in range(self.detokenizer.max_length - 1):\n",
    "            decoder_input = np.pad(tokenized_results, (0, self.detokenizer.max_length - len(tokenized_results)), constant_values=0).astype(np.int64)\n",
    "            input_dict = {\n",
    "                self.model._inputs_meta[0].name: np.expand_dims(encoder_input, axis=0),\n",
    "                self.model._inputs_meta[1].name: np.expand_dims(decoder_input, axis=0),\n",
    "            }\n",
    "            preds = self.model.run(None, input_dict)[0] # preds shape (1, 206, 29110)\n",
    "            pred_results = np.argmax(preds, axis=2)\n",
    "            tokenized_results.append(pred_results[0][index])\n",
    "\n",
    "            if tokenized_results[-1] == self.detokenizer.end_token_index:\n",
    "                break\n",
    "\n",
    "        results = self.detokenizer.detokenize([tokenized_results])\n",
    "        return results[0], time.time() - start\n",
    "\n",
    "def read_files(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        en_train_dataset = f.read().split(\"\\n\")[:-1]\n",
    "    return en_train_dataset\n",
    "\n",
    "# Path to dataset\n",
    "en_validation_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-dev.en\"\n",
    "es_validation_data_path = \"/content/drive/MyDrive/Colab Notebooks/opus.en-fa-dev.fa\"\n",
    "\n",
    "en_validation_data = read_files(en_validation_data_path)\n",
    "es_validation_data = read_files(es_validation_data_path)\n",
    "\n",
    "# Consider only sentences with length <= 500\n",
    "max_lenght = 500\n",
    "val_examples = [[es_sentence, en_sentence] for es_sentence, en_sentence in zip(es_validation_data, en_validation_data) if len(es_sentence) <= max_lenght and len(en_sentence) <= max_lenght]\n",
    "\n",
    "translator = PtEnTranslator(\"/content/drive/MyDrive/Colab Notebooks/Models/202408181252/model.onnx\")\n",
    "\n",
    "val_dataset = []\n",
    "for es, en in val_examples:\n",
    "    results, duration = translator.predict(es)\n",
    "    print(\"Farsi:     \", es.lower())\n",
    "    print(\"English:     \", en.lower())\n",
    "    print(\"English pred:\", results)\n",
    "    print(duration)\n",
    "    print()"
   ],
   "metadata": {
    "id": "haYJYXH8CPDG",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "bdf0ed2e-8c53-4a7a-e1bf-fcc55e418b70",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Farsi:      و گفتند: جز زندگى دنيوى ما هيچ نيست. مى‌ميريم و زنده مى‌شويم و ما را جز دهر هلاك نكند. آنان را بدان دانشى نيست و جز در پندارى نيستند.\n",
      "English:      and they say: there is naught but our life of the world; we die and we live, and naught destroyeth us save time; when they have no knowledge whatsoever of (all) that; they do but guess.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.1427109241485596\n",
      "\n",
      "Farsi:      من فرستادمتون که يه نوع مواد مخدر جديد کشف کنيد و شما با سس گوجه فرنگي برگشتين\n",
      "English:      i send you out for exciting new designer drugs, and you come back with tomato sauce.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.145071029663086\n",
      "\n",
      "Farsi:      - ماهي و گوشت نميخورم .\n",
      "English:      - i don't eat meat or fish.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.6095120906829834\n",
      "\n",
      "Farsi:      بهتره که ما یک فرصت طولانیتری به شما بدیم.»\n",
      "English:      we'd really better give you a longer sound bite.\"\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.263593912124634\n",
      "\n",
      "Farsi:      خيلي خب ، من نميدونم تو واقعا کي هستي اما همينطوري ميتونم حدس بزنم که تو هيچ تمريني براي شرکت در جنگ نکردي درسته؟\n",
      "English:      well, i do not know who you really are, but by the way you look, i can see you have no experience in war.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.1205525398254395\n",
      "\n",
      "Farsi:      ولي... من بودم تقصير اون نمينداختم.\n",
      "English:      i wouldn't want her blamed.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.1507081985473633\n",
      "\n",
      "Farsi:      اون براي ورزشکار و ستاره راک اندرول هم شوم بوده. اون مهارت زيادي در گول زدن مردم داره.\n",
      "English:      she fucks fighters and rock and roll stars and she's got a degree in screwing with people's heads.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.1630542278289795\n",
      "\n",
      "Farsi:      من واسه آقاي کاروس دارم\n",
      "English:      i have something for monsieur karos.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.144704580307007\n",
      "\n",
      "Farsi:      wear this necklace, and i shall recognize you.\n",
      "English:      wear this necklace, and i shall recognize you.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.394472360610962\n",
      "\n",
      "Farsi:      خودش بعد مدتي ازم تشكر ميكنه،لعنتي\n",
      "English:      what he will do is he will thank me in the long run.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.2776377201080322\n",
      "\n",
      "Farsi:      باورم نميشه تو 10 سال گذشته چنين احمقي نديده بودم\n",
      "English:      i don't believe this! i've never seen such an idiot in 10 years!\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.145371437072754\n",
      "\n",
      "Farsi:      فکر میکنم اون تیکه گوشتت آماده ست.\n",
      "English:      i think your bacon's ready.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.17274808883667\n",
      "\n",
      "Farsi:      زيبايي وقت ميبره يه دفعه که رخ نميده\n",
      "English:      beauty takes time. this don't just happen.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.131700038909912\n",
      "\n",
      "Farsi:      جهت ياد آوري بايد بگم من فعلا درگيرِ يه رابطه ادراي-رفاقتي هستم\n",
      "English:      for the record, i am involved in a very taboo interoffice relationship.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.1940577030181885\n",
      "\n",
      "Farsi:      اين دليليه که من سعي نمي کنم به مردم نزديک بشم\n",
      "English:      this ls why i try not to get close to people.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.6228585243225098\n",
      "\n",
      "Farsi:      -بس کن به گه مي کشمشون.\n",
      "English:      - stop shifting in your seat.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.130056619644165\n",
      "\n",
      "Farsi:      بنظر خيلي به خودتون افتخار ميکنين.\n",
      "English:      you seem proud of yourself.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.1166136264801025\n",
      "\n",
      "Farsi:      پس [مريم‌] در حالى كه او را در آغوش گرفته بود به نزد قومش آورد. گفتند: «اى مريم، به راستى كار بسيار ناپسندى مرتكب شده‌اى.»\n",
      "English:      then she brought him to her people, carrying him. they said, \"o mary, you have certainly done a thing unprecedented.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.13611102104187\n",
      "\n",
      "Farsi:      و این شخص جز آنکه مردی است که دروغ و افترا بر خدا می‌بندد هیچ مزیّت ندارد و ما هرگز به او ایمان نخواهیم آورد.\n",
      "English:      he is nothing but a man, making up lies about god. we have no faith in him.”\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.1486754417419434\n",
      "\n",
      "Farsi:      به لانکفورد زنگ زدم.\n",
      "English:      - alright, call lankford, have him pull up roulet's driving record.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.2376856803894043\n",
      "\n",
      "Farsi:      خدايا ، واقعا تو اين کار مزخرفم\n",
      "English:      jeez, i suck at this.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.4139578342437744\n",
      "\n",
      "Farsi:      . من تازه اومدم دانشگاه - هی ، نوشیدنی من کجاست ؟\n",
      "English:      -are you in college? -hey! where's my drink?\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.104799747467041\n",
      "\n",
      "Farsi:      همانها که (مردم را) از راه خدا بازمی‌دارند؛ و راه حق را کج و معوج نشان می‌دهند؛ و به سرای آخرت کافرند!\n",
      "English:      who debar (men) from the way of allah and would have it crooked, and who are disbelievers in the hereafter.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.460958480834961\n",
      "\n",
      "Farsi:      حالا به مليسا چي بگم ؟\n",
      "English:      what am i gonna tell melissa?\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.401719808578491\n",
      "\n",
      "Farsi:      حداقل با مرگم باعث میشم پدر و مادرم سر شامی که قراره از دست بدم موضوعی برای حرف زدن داشته باشن\n",
      "English:      at least my death will give my parents something to talk about at the meal i'm gonna miss.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.1189043521881104\n",
      "\n",
      "Farsi:      -دروغ نگو، من به تو چيزي نگفتم .\n",
      "English:      - not true, i didn't tell you anything.\n",
      "English pred:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "2.3318958282470703\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-25-1733cb553c60>\u001B[0m in \u001B[0;36m<cell line: 57>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[0mval_dataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0men\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mval_examples\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m     \u001B[0mresults\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mduration\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtranslator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     59\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Farsi:     \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mes\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"English:     \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0men\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-25-1733cb553c60>\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, sentence)\u001B[0m\n\u001B[1;32m     26\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_inputs_meta\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexpand_dims\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdecoder_input\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m             }\n\u001B[0;32m---> 28\u001B[0;31m             \u001B[0mpreds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_dict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;31m# preds shape (1, 206, 29110)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m             \u001B[0mpred_results\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpreds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m             \u001B[0mtokenized_results\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpred_results\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, output_names, input_feed, run_options)\u001B[0m\n\u001B[1;32m    218\u001B[0m             \u001B[0moutput_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0moutput\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_outputs_meta\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    219\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 220\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sess\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_names\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_feed\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_options\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    221\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mC\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEPFail\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    222\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_enable_fallback\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}